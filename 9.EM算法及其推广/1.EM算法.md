# EM算法

1. EM算法是一种迭代算法，用于解决含有隐变量(hidden varialbe)的混合模型的参数估计问题，即解决极大似然问题。

   > 一般来说，对极大似然估计(MLE)问题，对于$P(x|\theta)$，其中$\theta$为参数，极大似然估计就是求：
   > $$
   > \theta_{max}=\arg \max_{\theta} P(x|\theta)
   > $$
   > 通常使用对数似然函数，那么即是求：
   > $$
   > \theta_{max}=\arg\max_{\theta}\log{P(x|\theta)}
   > $$

   

2. EM算法的每次迭代由两部分组成：

   + E步：求期望(expectation)。
   + M步：求极大(maximization)。

   因此，EM算法也被称为期望极大算法(expectation maximization algorithm)。

## 一、EM算法

1. EM算法的公式：
   $$
   \theta^{(t+1)}=\arg\max_{\theta}\int_{z}\log{P(x,z|\theta).P(z|x,\theta^{(t)})}\mathrm{dz}
   $$
   其中，$z$为隐变量，$P(z|x,\theta^{(t)})$是后验概率，$\log{P(x,z|\theta)}$称为对数联合概率(对数完全数据，其中$x,z$是完全数据)。

   而这实际上就是：
   $$
   \theta^{(t+1)}=\arg \max_{\theta} E_{z|x,\theta^{(t)}}[\log P(x,z|\theta)]
   $$
   这对应着EM算法每次迭代由两部分组成：

   + E步：求期望(expection)。
   + M步：求期望的极大(maximization)。

2. 定义Q函数：Q函数是完全数据的对数似然函数$\log(P(x,z|\theta))$关于在给定观测数据x和当前参数$\theta^{(i)}$下对未观测数据z的条件概率分布$P(z|x,\theta^{(i)})$，即：
   $$
   Q(\theta,\theta^{(i+1)})=\int_z\log(P(x,z|\theta)).P(z|x,\theta^{(i)})\mathrm{dz}\\
   =E_{z|x,\theta^{(i)}}[\log(P(x,z|\theta))]
   $$



## 二、相对熵(relative entropy)

相对熵，也称KL散度(Kullback-Leibler divergence)，是两个概率分布之间差异的非对称性度量。

> 相对熵是EM算法中的损失函数，此时参与计算的一个概率分布为真实分布，另一个为理论分布 (拟合分布)。

设$p(x)、q(x)$是离散随机变量X中取值的两个概率分布，则p对q的相对熵是：
$$
D_{KL}(p||q)=\sum_xp(x)\log(\frac{p(x)}{q(x)})=E_{p(x)}\log(\frac{p(x)}{q(x)})
$$
性质：

1. 如果$p(x),q(x)$的分布相同，那么相对熵等于0。

2. $D_{KL}(p||q)\neq D_{KL}(q||p)$，相对熵具有不对称性。

3. $D_{KL}(p||q)\ge0$。

   > 证明：
   > $$
   > \begin{aligned}
   > D_{KL}(p||q)&=\sum_xp(x)\log(\frac{p(x)}{q(x)})\\
   > &=-\sum_xp(x)\log(\frac{q(x)}{p(x)})\\
   > &=-E_{p(x)}(\log(\frac{q(x)}{p(x)}))\\
   > &[利用jenson不等式在概率论中的形式：f(E[x])\le E[f(x)]]\\
   > &\ge -\log E_{p(x)}(\frac{q(x)}{p(x)})\\
   > &=-\log \sum_x p(x)\frac{q(x)}{p(x)}\\
   > &=-\log\sum_x q(x)
   > \end{aligned}
   > $$
   > 又因为:
   > $$
   > \sum_xp(x)=1
   > $$
   > 所以：
   > $$
   > D_{KL}(p||q)\ge0
   > $$

   

## 三、EM算法收敛性

1. **定理1**：设$P(x|\theta)$为观测数据的似然函数，$\theta^{(i)}(i=1,2,...)$为EM算法得到的参数估计序列，$P(x|\theta^{(i)})$为对应的似然函数序列，则$P(x|\theta^{(i)})$是单调递增的，即:
   $$
   P(x|\theta^{(i+1)})\ge P(x|\theta^{(i)})
   $$

   > 证明：
   >
   > 因为
   > $$
   > P(x|\theta)=\frac{P(x,z|\theta)}{P(z|x,\theta)}
   > $$
   > 因此
   > $$
   > \log(P(x|\theta))=\log(P(x,z|\theta))-\log(P(z|x,\theta))
   > $$
   > 对左右两边分别积分：
   > $$
   > 左边=\int_z P(z|x,\theta^{(t)}).\log(P(x|\theta))\mathrm{dz}\\
   > =\log(P(x|\theta))\int_z P(z|x,\theta^{(t)})\mathrm{dz}\\
   > =\log(P(x|\theta))
   > $$
   >
   > $$
   > 右边=\int_zP(z|x,\theta^{(t)}).\log(P(x,z|\theta))\mathrm{dt}-\int_zP(z|x,\theta^{(t)}).\log(P(z|x,\theta))\mathrm{dz}
   > $$
   >
   > 
   >
   > 