# 感知机学习策略

## 一、数据集的线性可分性

1. **数据集的线性可分性：**给定一个数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$。其中，$x_i \in X \subseteq R^n，y \in Y ={+1,-1},i=1,2,...,N$，如果存在某个超平面$S:w.x+b=0$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例i，有$w.x_i+b>0$，对所有$y_i=-1$的实例i，有$w.x_i+b<0$，则称数据集T为线性可分数据集(linearly separable data set)。否则，称数据集T为线性不可分。



## 二、感知机学习策略

1. 假设训练数据集是线性可分的，感知机学习的目标是**求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面**。即确定感知机模型参数$w,b$。

2. 感知机所采用的**损失函数**是**误分类点到超平面S的总距离。**

   输入空间$R^n$中任一点$x_0$到超平面S的距离：$\frac{1}{||w||}|w.x_0+b|$。

   这里$||w||$是$w$的$L_2$范数。

3. 其次，对于误分类的数据$(x_i,y_i)$来说，$-y_i(w.x_i+b)>0$成立，证明是显然的。

   当$w.x_i+b>0$时，$y_i=-1$，而当$w.x_i+b < 0$时，$y_i=+1$。

   因此，误分类点$(x_i,y_i)$到超平面的距离是$-\frac{1}{||w||}y_i(w.x_i+b)$。

4. 假设超平面S的误分类点集合为M，那么所有误分类点到超平面S的总距离为
   $$
   -\frac{1}{||w||}\sum_{x_i\in M}y_i(w.x_i+b)
   $$
   不考虑$\frac{1}{||w||}$，就得到感知机学习的损失函数。

5. 因此，给定训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$。其中，$x_i\in X =R^n,y_i \in Y = {+1,-1}，i=1,2,...N$。感知机$sign(w.x+b)$学习的损失函数定义为：
   $$
   L(w,b)=-\sum_{x_i \in M}y_i(w.x_i+b)
   $$
   其中M为误分类点的集合，这个损失函数就是感知机学习的经验风险函数。

6. + 显然，损失函数$L(w,b)$是非负的，如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。
   + 一个特定的样本点的损失函数：在误分类时是参数$w,b$的线性函数，在正确分类时是0。
   + 因此，**给定训练数据集T，损失函数$L(w,b)$是$w,b$的连续可导函数**。





