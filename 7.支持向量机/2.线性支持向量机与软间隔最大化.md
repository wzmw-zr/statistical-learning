# 线性支持向量机与软间隔最大化

## 一、线性支持向量机

1. 给定一个特征空间上的训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$。其中，$x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N\}$，$x_i$为第i个特征向量，$y_i$为$x_i$的类标记。再假设训练数据集不是线性可分的。通常情况是，训练数据有一些特异点(outlier)，将这些特异点去除后，剩下大部分的样本点组成的集合是线性可分的。

2. 线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点引进一个松弛变量$\delta_i\ge0$，使函数间隔加上松弛变量大于等于1。这样，约束条件变为$y_i(w.x_i+b)\ge1-\delta_i$。

3. 同时，对每个松弛变量$\delta_i$，支付一个代价$\delta_i$。目标函数由原来的$\frac{1}{2}||w||^2$变成
   $$
   \frac{1}{2}||w||^2+C\sum_{i=1}^N \delta_i
   $$
   这里，$C>0$称为惩罚参数。C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。

   最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和两者的系数。

4. 线性不可分的线性支持向量机的学习问题就变成了凸二次规划(convex quadratic programming)问题(原始问题)：
   $$
   \min_{w,b,\delta}\frac{1}{2}||w||^2+C\sum_{i=1}^N\delta_i\\
   \mathrm{s.t.}
   \left.
   \begin{array}{**lr**}
   &y_i(w.x_i+b)\ge1-\delta_i,\quad i=1,2,...,N\\
   &\delta_i\ge0,\quad i=1,2,...,N
   \end{array}
   \right.
   $$
   原始问题是一个凸二次规划问题，因而关于$(w,b,\delta)$的解是存在的。可以证明$w$的解是唯一的，但是$b$的解可能不唯一，而是存在于一个区间。

5. 设问题的解是$(w^*,b^*)$，于是可以得到分离超平面$w^*.x+b^*=0$和分类决策函数$f(x)=\mathrm{sign}(w^*.x+b^*)$。

6. **线性支持向量机**：对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题，得到的分离超平面为$w^*.x+b^*=0$，以及相应的分类决策函数$f(x)=\mathrm{sign}(w^*.x+b^*)$，称为线性支持向量机。



## 二、学习的对偶算法

1. 原始最优化问题的拉格朗日函数是：
   $$
   L(w,b,\delta,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^N \delta_i-\sum_{i=1}^N\alpha_i(y_i(w.x_i+b)-1+\delta_i)-\sum_{i=1}^N\mu_i\delta_i
   $$
   其中$\alpha_i\ge0,\delta_i\ge0$。

   对偶问题是拉格朗日函数的极大极小问题。首先求$L(w,b,\delta,\alpha,\mu)$对$w,b,\delta$的极小，由：
   $$
   \nabla_wL(w,b,\delta,\alpha,\mu)=w-\sum_{i=1}^N\alpha_iy_ix_i=0\\
   \nabla_bL(w,b,\delta,\alpha,\mu)=-\sum_{i=1}^N\alpha_iy_i=0\\
   \nabla_{\delta_i}=C-\alpha_i-\mu_i=0
   $$
   得：
   $$
   w=\sum_{i=1}^N\alpha_iy_ix_i\\
   \sum_{i=1}^N\alpha_iy_i=0\\
   C-\alpha_i-\mu_i=0
   $$
   代入$L(w,b,\delta,\alpha,\mu)$得：
   $$
   L(w,b,\delta,\alpha,\mu)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i.x_j)+\sum_{i=1}^N \alpha_i
   $$
   再对$\min_{w,b,\delta}L(w,b,\delta,\alpha,\mu)$求$\alpha$的极大，即得对偶问题：
   $$
   \max_{\alpha}-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i.x_j)+\sum_{i=1}^N\alpha_i\\
   \mathrm{s.t.}
   \sum_{i=1}^N\alpha_iy_i=0\\
   C-\alpha_i-\mu_1=0\\
   \alpha_i\ge0\\
   \mu_i\ge0,\quad i=1,2,...,N
   $$

2. 通过求解对偶问题而得到原始问题的解，进而确定分离超平面和决策函数。

   原始问题的最优解与对偶问题的最优解的关系：

   设$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$是对偶问题的一个解，若存在$\alpha^*$的一个分量$\alpha_j^*$，$0<\alpha_j^*<C$，则原始问题的解$w^*,b^*$可以按照下式求得：
   $$
   w^*=\sum_{i=1}^N\alpha_i^*y_ix_i\\
   b^*=y_j-\sum_{i=1}^Ny_i\alpha_i^*(x_i.x_j)
   $$
   因此，分离超平面为:
   $$
   \sum_{i=1}^N\alpha_i^*y_ix_i.x+b^*=0
   $$
   分类决策函数是：
   $$
   f(x)=\mathrm{sign}(\sum_{i=1}^N\alpha_i^*y_ix_i.x+b^*)
   $$



## 三、线性支持向量机的学习算法

1. **输入**：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。

2. **输出**：分离超平面和分类决策函数。

3. **步骤**：

   + 选择惩罚参数$C>0$，构造并求解凸二次规划问题：
     $$
     \min_{\alpha}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i.x_j)-\sum_{i=1}^N\alpha_i\\
     \mathrm{s.t.}\sum_{i=1}^N\alpha_iy_i=0\\
     0\le \alpha_i\le C,i=1,2,...,N
     $$
     求得最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)$。

   + 计算$w^*=\sum_{i=1}^N\alpha_i^*y_ix_i)$，选择$\alpha^*$的一个分量$\alpha_j^*$适合条件$0<\alpha_j^*<C$，求得$b^*=y_j-\sum_{i=1}^Ny_i\alpha_i^*(x_i.x_j)$。

   + 求得分离超平面$w^*.x+b^*=0$，分类决策函数$f(x)=\mathrm{sign}(w^*.x+b^*)。$



## 四、支持向量

1. 在线性不可分的情况下，将对偶问题的解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$中对应于$\alpha^*>0$的样本点$(x_i,y_i)$实例$x_i$称为支持向量(软间隔支持向量)。
2. 软间隔支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。
   + 若$\alpha_i^*<C$，则$\delta_i=0$，支持向量$x_i$恰好落在间隔边界上;
   + 若$\alpha_i^*=C，0 < \delta_i < 1$，则分类正确，$x_i$在间隔边界与分离超平面之间;
   + 若$\alpha_i^*=C,\delta_i=1$，则$x_i$在分离超平面上;
   + 若$\alpha_i^*=C,\delta_i>1$，则$x_i$位于分离超平面误分类一侧。



