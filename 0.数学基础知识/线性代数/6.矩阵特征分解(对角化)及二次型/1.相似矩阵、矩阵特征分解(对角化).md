# 相似矩阵、矩阵对角化

## 一、相似矩阵

$A_{n\times n}$和$B_{n \times n}$相似，当且仅当存在可逆矩阵$P_{n \times n}$，使得$P^{-1}AP=B$。可以证明，相似即等价。



## 二、矩阵特征分解(对角化)

假设**方阵**$A_{n\times n}$有n个特征值$\lambda_1\le\lambda_2\le...\le\lambda_n$和特征向量$x_1,x_2,...,x_n$。

令$S=[x_1,x_2,...,x_n]$，则$AS=A[x_1,x_2,...,x_n]=[\lambda_1x_1,\lambda_2x_2,...,\lambda_nx_n]$。

令：
$$
\Lambda=\left[
\begin{matrix}
\lambda_1 & & \\
& \ddots & \\
& & \lambda_n
\end{matrix}
\right]
$$
则$S\Lambda=[\lambda_1x_1,\lambda_2x_2,...\lambda_nx_n]$。

故：$S^{-1}AS=\Lambda \Leftrightarrow A=S\Lambda S^{-1}$。

**对角化A的两个前提条件：**

+ **==A是方阵。==**
+ **S可逆，就是==有N个线性无关的特征向量==，k重特征值必定可以找到k个线性无关的特征向量。**

**矩阵对角化的意义在于可以实现矩阵压缩，减小神经网络中算法的复杂度。**

> 当矩阵A是一个对称矩阵时，则存在一个对角化分解，即$A=S\Lambda S^T$，其中S的每一列都是相互正交的特征向量，且是单位向量，$\Lambda$对角线上的元素是从大到小·排列的特征值。
>
> 一般我们会将特征向量标准化，用施密特正交化算法即可，此时$SS^T=S^TS=E$，即$S^T=S^{-1}$。

> 更为普遍的，对于行列数量不同的，可以使用SVD(奇异值分解)。

>**定理1：**n阶方阵A可相似对角化的充要条件为A有n个线性无关的特征向量。
>
>**定理2**：n阶实对称矩阵必定能相似对角化。
>
>**定理3**：n阶实对称矩阵，矩阵的秩$r(A)=k(0< k <n)$，则$\lambda=0$恰为A的$(n-k)$重特征值。

