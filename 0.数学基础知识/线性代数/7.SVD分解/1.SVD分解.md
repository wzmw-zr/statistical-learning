# SVD分解

特征分解的局限性太大，在深度学习中，SVD分解应用较多，通常用来压缩图像、做主成分分析等。

## 一、SVD分解(signular value decomposition)

SVD对矩阵进行分解，但不像特征分解那样要求矩阵为方阵且特征值不重复。

当给定一个大小为$m\times n$的矩阵A，虽然矩阵A不一定是方阵，但是大小为$m\times m$的$AA^T$和大小为$n\times n$的$A^TA$却是对称矩阵。因为实对称矩阵必定是可对角化的，因此，设$AA^T=P\Lambda_1P^T,A^TA=Q\Lambda_2Q^T$，则矩阵的奇异值分解(singular value decomposition)为$A=P\sum Q^T$。

其中矩阵$P_{m\times m}=[p_1,p_2,..,p_m]$，$p_i$为$AA^T$的特征向量，也被称为矩阵A的左奇异值向量(left singular vector)。矩阵$Q=[q_1,q_2,...,q_n]$，$q_i$为$A^TA$的特征向量，也被称为矩阵A的右奇异值向量(right singular vector)。

矩阵${\Lambda_1}$的大小为$m\times m$，矩阵$\Lambda_2$的大小为$n \times n$，两个矩阵对角线上的非零元素相同，即$AA^T,A^TA$的非零特征值相同，矩阵$\sum$的大小为$m\times n$，其对角线上的元素被称为奇异值(singular value)。

## 二、SVD分解的意义

SVD分解的应用领域：

+ 隐性语义分析(Latent Semantic Analysis, LSA)或隐形语义索引(Latent Semantic Indexing，LSI)。
+ 推荐系统 (Recommender system)，可以说是最有价值的应用点；
+ 矩阵形式数据（主要是图像数据）的压缩。

**关于 SVD 的一些重要的结论性总结：**

- 任意的矩阵$A$是可以分解成三个矩阵；
- $P$ 表示了原始域的标准正交基；
- $Q$ 表示经过$A$变换后的新标准正交基；
- $\sum$ 表示了 $P$ 中的向量与$Q$中相对应向量之间的比例（伸缩）关系；
- $\sum$ 中的每个 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 会按从大到小排好顺序，值越大代表该维度重要性越高；

> numpy中的linalg模块实现了svd分解的函数，直接使用就行。