# 向量线性相关、內积、范数、正交、规范正交基

## 一、线性相关、线性无关

1. 线性相关：向量空间中的一组向量，如果其中一个向量可以由其他向量线性组合表示，则称这组向量线性相关。即对向量空间V中的一组向量$A:\alpha_1,\alpha_2,...\alpha_m$，存在不全为0的数$k_1,k_2,...,k_m$，使得$\sum\limits_{i=1}^mk_i\alpha_i=0$。

2. 线性无关：向量空间中的一组向量，如果没有一个向量可以由其他向量线性组合表示，则称这组向量线性无关。即对向量空间V中的一组向量$A:\alpha_1,\alpha_2,...\alpha_m$，只有数$k_1,k_2,...,k_m$全为0，才能使得$\sum\limits_{i=1}^mk_i\alpha_i=0$。

3. 性质：

   + 原本的向量组是线性相关的，增加向量的个数，不改变向量的相关性。
   + 原本的向量组是线性无关的，减少向量的个数，不改变向量的无关性。
   + 一个向量组线性无关，则在相同位置处都增加一个分量后得到的新向量组仍线性无关。
   + 一个向量组线性相关，则在相同位置处都去掉一个分量后得到的新向量组仍线性相关。
   + 若向量组所包含向量个数等于分量个数时，判定向量组是否线性相关即是判定这些向量为列组成的行列式是否为零。若行列式为零，则向量组线性相关；否则是线性无关的。
   + n+1个n维向量总是线性相关。

   

## 二、向量的內积与正交

$\alpha=(\alpha_1,\alpha_2,...,\alpha_n),\beta=(\beta_1,\beta_2,...,\beta_n)$，则$\alpha \cdot \beta=\sum\limits_{i=1}^n\alpha_i\beta_i$。

內积为0的两个非0向量正交。

标准正交基：內积为0的、长度为1的一组向量空间的基。

**正交矩阵**：行向量、列向量分别是标准正交的方阵，即$AA^T=A^TA=E$，即$A^T=A^{-1}$。

> 正交矩阵的意义。。。求逆的代价小？



## 三、施密特正交化

根据一组向量空间的基$(v_1,v_2,..,v_n)$计算一组正交基$(\beta_1,\beta_2,...\beta_n)$：
$$
\begin{matrix}
\beta_1=v_1,&\eta_1=\frac{\beta_1}{||\beta_1||}\\
\beta_2=v_2-(v_2\cdot\eta_1)\eta_1,&\eta_2=\frac{\beta_2}{||\beta_2||}\\
\beta_3=v_3-(v3\cdot\eta_1)\eta_1-(v3\cdot \eta_2)\eta_2,&\eta_3=\frac{\beta_3}{||\beta_3||}\\
\vdots &\vdots\\
\beta_n=v_n-\sum_{i=1}^{n-1}(v_i\cdot\eta_i)\eta_i,&\eta_n=\frac{\beta_n}{||\beta_n||}
\end{matrix}
$$


## 四、向量的范数

**范数(norm)，是具有长度概念的函数**，用于衡量向量的大小。

形式上，$L^p$范数定义如下：
$$
||x||_p=(\sum_{i}|x_i|^p)^{\frac{1}{p}}
$$
其中$p\in R,p \ge 1$。

范数是将向量映射到非负值的函数。严格来说，范数是满足下列性质的函数：

+ $f(\boldsymbol{x})=0\Leftrightarrow \boldsymbol{x}=\boldsymbol{0}$。
+ $f(\boldsymbol{x}+\boldsymbol{y})\le f(\boldsymbol{x})+f(\boldsymbol{y})$(三角不等式(triangle inequality))。
+ $\forall \alpha \in R, f(\alpha \boldsymbol{x})=|\alpha|f(\boldsymbol{x})$。

对$x=(x_1,x_2,...,x_n)$，常见的范数：

+ L1-范数：$||x||_1=\sum\limits_{i=1}^n|x_i|$。
+ L2-范数：$||x||_2=(\sum\limits_{i=1}^nx_i^2)^\frac{1}{2}$。
+ Lp-范数：$||x||_p=(\sum\limits_{i=1}^nx_i^p)^{\frac{1}{p}}$。
+ L$\infty$-范数：$||x||_{\infty}=\max\limits_i|x_i|$。
+ L$-\infty$-范数：$||x||_{-\infty}=\min\limits_{i}|x_i|$。 



在深度学习中有时希望衡量矩阵的大小，就会使用$Frobenius \;norm$，即$||A||_F=(\sum\limits_{i,j}A_{i,j}^2)^{\frac{1}{2}}$。