# 偏导数、方向导数、梯度、多元复合函数求导法则

## 一、偏导数

设$D\subset R^n$为开集，$f(x_1,x_2...,x_n),\;\;(x_1,x_2,...,x_n)\in D$是定义在$D$上的n元函数，$(x_1^0, x_2^0,...,x_n^0)$是D中的一定点。如果存在极限：
$$
\lim_{\triangle x\rightarrow 0}\frac{f(x_1^0,x_2^0,...,x_i^0+\triangle x,...x_n^0)-f(x_1^0,x_2^0,...,x_n^0)}{\triangle x}
$$
那么就称函数$f$在点$(x_1^0,x_2^0,...,x_n^0)$关于$x_i$可偏导，并称此极限为$f$在点$(x_1^0,x_2^0,...,x_n^0)$关于$x_i$的偏导数。

如果函数$f$在D中每一个点都关于x可偏导，则D中每一个点$(x,y)$与其相对应的$f$关于$x_i$的偏导数，记为$\frac{\partial f(x_1,x_2,...,x_n)}{\partial x_i}$或$f_{x_i}(x_1,x_2,...,x_n)$。



## 二、方向导数

设$f:D\rightarrow R,\; D \subset R^n$，给定D内某点$\boldsymbol=(x_1,x_2,...,x_n)$以及任意非零向量$\boldsymbol{v}(v_1,v_2,...,v_n)$，定义：
$$
f_v(t)=f(\boldsymbol{x}+t\boldsymbol{v})
$$
若$f_v$对$t$的微分在$t=0$处存在，那么可以定义在$\boldsymbol{x}$沿$\boldsymbol{v}$的方向导数为：
$$
\nabla_vf(\boldsymbol{x})=\frac{\mathrm{d}f_v}{\mathrm{d}t}|_{t=0}=\lim_{t\rightarrow 0}\frac{f(\boldsymbol{x}+t\boldsymbol{v})-f(\boldsymbol{x})}{t}
$$


## 三、全微分

设$D\subset R^n$是开集，$f(x_1,x_2,...,x_n),\;\; (x_1,x_2,...,x_n)\in D$，$(x_1^0,x_2^0,...,x_n^0)$是$D$内的一定点，若存在只与$(x_1^0,x_2^0,...,x_n^0)$有关的常数$A_1,A_2,...,A_n$，使得:
$$
\triangle z=A_1\triangle x_1+A_2\triangle x_2+...+A_n\triangle x_n+o(\sqrt{\triangle x_1^2+...\triangle x_n^2})
$$
我们称函数$f$在点$(x_1^0,x_2^0,...x_n^0)$处是可微的。可以证明可微必可偏导。

> 全微分用的很少，基本上都使用偏微分。



## 四、梯度

设$D\subset R^n$是开集，$f(x_1,x_2,...,x_n),\;\; (x_1,x_2,...,x_n)\in D$，$(x_1^0,x_2^0,...,x_n^0)$是$D$内的一定点。如果函数$f$在点$(x_1^0,x_2^0,...,x_n^0)$可偏导，则称向量$(f_{x_1}(x_1^0,...,x_n^0),...,f_{x_n}(x_1^0,x_2^0,...,x_n^0))$为$f$在点$(x_1^0,x_2^0,...,x_n^0)$的梯度，记做$\boldsymbol{\mathrm{grad}} f(x_1^0,x_2^0,...x_n^0) $，可以看做是向量。

> 梯度决定了多元函数的单调性和极值，几乎所有的连续优化算法都需要计算函数的梯度值，以寻找梯度为0的点作为目标。



## 五、高阶偏导

**定理**：如果函数$f(x_1,x_2,...,x_n)$的两个混合偏导数$f_{x_{i1}x_{i2}...x_{ik}}$和$f_{x_{j1}x_{j2}...x_{jk}}$在点$(x_1^0,x_2^0,...,x_n^0)$连续，且$x_{i1}x_{i2}...x_{ik}$与$x_{j1}x_{j2}...x_{jk}$只是排列顺序不同，则：
$$
f_{x_{i1}x_{i2}...x_{ik}}(x_1^0,x_2^0,...x_n^0)=f_{x_{j1}x_{j2}...x_{jk}}(x_1^0,x_2^0,...,x_n^0)
$$

> 实际上就是什么情况下偏导的顺序不影响结果。。。光有梯度值还是无法确定函数的极值。



## 六、向量值函数

$\boldsymbol{f}:D\rightarrow R^m,\; D \subset R^n$，实际上就是一组函数$\boldsymbol{f}=[f_1,f_2,...f_m]^T\;\;(f_i:D\rightarrow R,i=1,...,m)$。

若$\boldsymbol{f}$的每一个分量函数$f_i(x_1,x_2,...,x_n)(i=1,2,...m)$都在$\boldsymbol{x^0}$点可偏导，就称向量值函数$\boldsymbol{f}$在$\boldsymbol{x^0}$点可导，称矩阵：
$$
J(f)=[\frac{\partial\boldsymbol{f}}{\partial x_1},\frac{\partial\boldsymbol{f}}{\partial x_2},...\frac{\partial\boldsymbol{f}}{\partial x_n}]=\left[
\begin{matrix}
\frac{\partial f_1}{\partial x_1} &\cdots & \frac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\frac{\partial f_m}{\partial x_1} &\cdots & \frac{\partial f_m}{\partial x_n}\\
\end{matrix}
\right]
$$
为向量值函数$\boldsymbol{f}$在$\boldsymbol{x^0}$处的导数，或者Jacobian矩阵，每行是对应的分量函数的梯度。

> Jacobian矩阵体现了一个可微方程与给出点的最优线性逼近，简化了向量值函数的求导，在神经网络的反向传播算法中使用广泛。



## 七、多元复合函数求导法则(链式求导法则)

设$\boldsymbol{f}:D\rightarrow R^m,\; D \subset R^n$，$\boldsymbol{g}:U\rightarrow R,\; U \subset R^m$(f值域是U的子集)，则可以构造复合函数$z=g\circ\boldsymbol{f}=g(f_1(x_1,x_2,...,x_n),...,f_m(x_1,x_2,...,x_n))$。设$\boldsymbol{f}$在$\boldsymbol{x_0}$可导，即$f_1,f_2,...,f_m$在$\boldsymbol{x_0}$可偏导，设$\boldsymbol{y_0}=[y_1,y_2,...y_m]=[f_1(\boldsymbol{x_0}),...,f_m(\boldsymbol{x_0})]$，若$g$在$\boldsymbol{y_0}$可微，那么：
$$
\frac{\partial z}{\partial x_i}=\boldsymbol{\mathrm{grad}}z(\boldsymbol{y_0})\frac{\partial \boldsymbol{y_0}}{\partial x_i}=\sum_{j=1}^m\frac{\partial z}{\partial y_j}(\boldsymbol{y_0})\frac{\partial y_j}{\partial x_i}(\boldsymbol{x_0})
$$

> 各个神经网络的反向传播算法都依赖于链式法则。