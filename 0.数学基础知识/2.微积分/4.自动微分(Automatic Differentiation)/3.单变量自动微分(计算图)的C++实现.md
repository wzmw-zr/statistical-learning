# 单变量自动微分(计算图)的C++实现

tensorflow，pytorch中的自动微分都是基于张量的，这里为了简单就只实现了单变量的自动微分，但是有助于对深度学习中的自动微分的计算图的理解。

因为在深度学习框架中的计算图中，事先定义好了节点的运算，这两就可以方便地进行符号微分，我们需要定义节点的结构，同时也需要定义这些运算，这些运算都是返回新的节点。

在建立好计算图之后，首先是前向传播计算出各个节点的值(这个可以在建立计算图的时候进行)，之后反向传播计算梯度(导数)，通过链式法则计算符合函数的微分(导数)。

关于反向传播，实际上就是一个拓扑排序的过程，一个节点的所有后继节点都被处理完了，这个节点的梯度就可以反向传给其父亲节点们了。

```c++
#include<iostream>
#include<string>
#include<vector>
#include<queue>
#include<stack>
#include<map>
#include<set>
#include<unordered_map>
#include<unordered_set>
#include<cmath>
#include<algorithm>
using namespace std;

// 操作接口类，即基本的函数，这里只定义了pow, exp, 加法，数乘的计算操作类, 为了方便我没有将析构函数声明为虚函数，实际上是需要的。
struct Op {
    virtual double grad() = 0;
};

struct Pow : public Op {
    double base;
    double power;

    Pow() = default;
    Pow(double base, double power) : base(base), power(power) {}

    double grad() override {
        return power * pow(base, power - 1);
    }
};

struct Exp : public Op {
    double power;
    Exp() = default;
    Exp(double power) : power(power) {}

    double grad() override {
        return exp(power);
    }
};

struct Add : public Op {
    double left;
    double right;
    Add() = default;
    Add(double left, double right) : left(left), right(right) {}

    double grad() override {
        return 1;
    }
};

struct Mul : public Op {
    double var;
    double times;
    Mul() = default;
    Mul(double var, double times) : var(var), times(times) {}

    double grad() override {
        return times;
    }
};

struct Single : public Op {
    double grad() override {
        return 1;
    }
};

struct Node {
    double val;
    double grad;
    Op *op;
    vector<Node *> forward;
    vector<Node *> backward;
    Node(double val) : val(val), grad(0.0), op(nullptr) {}
    Node(double val, Op *op) : val(val), grad(0.0), op(op) {} 
};

Node *my_pow(Node *pre, double power) {
    Op *op = new Pow(pre->val, power);
    Node *next = new Node(pow(pre->val, power), op);
    pre->forward.push_back(next);
    next->backward.push_back(pre);
    return next;
}

Node *my_exp(Node *pre) {
    Op *op = new Exp(pre->val);
    Node *next = new Node(exp(pre->val), op);
    pre->forward.push_back(next);
    next->backward.push_back(pre);
    return next;
}

Node *my_add(Node *left, Node *right) {
    Op *op = new Add(left->val, right->val);
    Node *next = new Node(left->val + right->val, op);
    left->forward.push_back(next);
    right->forward.push_back(next);
    next->backward.push_back(left);
    next->backward.push_back(right);
    return next;
}

Node *my_mul(Node *pre, double times) {
    Op *op = new Mul(pre->val, times);
    Node *next = new Node(pre->val * times, op);
    pre->forward.push_back(next);
    next->backward.push_back(pre);
    return next;
}

void dfs(Node *node, unordered_map<Node *, int> &mp) {
    if (!node) return ;
    mp[node] = node->forward.size();
    for (Node *x : node->backward) {
        if (mp.count(x)) continue;
        dfs(x, mp);
    }
}

// 反向传播
void backward(Node *node) {
    queue<Node *> que;
    que.push(node);
    // 获取反向的入度，实际上就是每个节点的forward的大小，后面进行拓扑排序
    unordered_map<Node *, int> mp;
    dfs(node, mp);
    node->grad = node->op->grad();
    while (!que.empty()) {
        Node *temp = que.front();
        que.pop();
        for (Node *x : temp->backward) {
            // 链式法则
            x->grad += x->op->grad() * temp->grad;
            if (!(--mp[x])) que.push(x);
        }
    }
}

int main() {
    Node *x = new Node(1, new Single()); // x
    Node *y = my_pow(x, 2); // x^2
    Node *z = my_mul(x, 3); // 3x
    Node *a = my_add(y, z); // x^2 + 3x
    Node *b = my_exp(a); // e^(x^2 + 3x)
    Node *c = my_add(b, z); // e^(x^2 + 3x) + 3x
    backward(c);
    cout << x->grad << endl;
    return 0;
}
```

这是一个计算$e^{x^2+3x}+3x$在$x=1$处的导数的程序，可以证明这是正确的。

这个简单的自动微分框架必然有很多缺陷，也许一些细节上会和正常的深度学习框架有区别，但是能够加深对自动微分的实现与执行过程的理解。