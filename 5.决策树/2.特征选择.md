# 特征选择

## 一、特征选择问题

1. + 特征选择在于**选取对训练数据具有分类能力的特征**。
   + 如果利用一个特征进行分类的结果与随机分类的结果没有很大的区别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。
   + 通常**特征选择的准则**是**信息增益**或者**信息增益比**。
2. 特征选择是决定用哪个特征来划分特征空间。
3. 直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。



## 二、信息增益(information gain)

### (1) 熵(entropy)与条件熵(condition entropy)

1. **熵**(entropy)在信息论与概率统计中，是表示**随机变量不确定性的度量**。设X是一个取有限个值的离散随机变量，其概率分布为 
   $$
   P(X=x_i)=p_i,i=1,2,...,n
   $$
   则随机变量X的熵定义为：
   $$
   H(X)=-\sum_{i=1}^np_i\log p_i
   $$
   若$p_i=0$，则定义$0 \log 0=0$。这时，熵的单位分别称作比特(bit)或钠特(nat)。**由定义可知，熵只依赖于X的分布，而与X的取值无关。**因此，也可以将X的熵记作$H(p)=-\sum_{i=1}^np_i\log p_i$。

   熵越大，随机变量的不确定性就越大，从定义可以验证：
   $$
   0\le H(p) \le \log n
   $$

2. 设有随机变量$(X,Y)$，其联合概率分布为$P(X=x_i,Y=y_j)=p_{ij},i=1,2,..,n; j=1,2,...,m$。

   **条件熵(condition entropy) $H(Y|X)$**表示**在已知随机变量X的条件下随机变量Y的不确定性**。

   随机变量X已知的条件下随机变量Y的条件熵$H(Y|X)$，定义为**X给定条件下Y的条件概率分布的熵对X的数学期**望：
   $$
   H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_i)
   $$
   这里，$p_i=P(X=x_i),i=1,2,..,n$。

3. 当熵和条件熵中的概率由数据估计(特别是极大似然顾及)得到时，所对应的熵与条件熵分别成为经验熵(empirical entropy)和经验条件熵(empirical condition entropy)。



### (2)信息增益(information gain)

1. **信息增益**(information gain)：

   特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$和特征A给定条件下的经验条件熵$H(D|A)$之差，即
   $$
   g(D,A)=H(D)-H(D|A)
   $$

2. 一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息(mutual information)。**决策树学习中的信息增益等价于训练数据集中类与特征的互信息。**

3. 决策树学习应用信息增益准则选择特征：

   给定训练数据集D和特征A，经验熵$H(D)$表示对数据集D进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征A给定的条件下对数据集D进行分类的不确定性。那么，它们的差，即**信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。**

   显然，信息增益依赖于特征，对于数据集D而言，不同的特征往往具有不同的信息增益**。信息增益大的特征具有更强的分类能力。**

4. 根据信息增益准则的特征选择方法：对训练数据集(或子集)D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。



### (3)信息增益算法

1. 设训练数据集为D，$|D|$表示其样本容量，即样本个数。设有K个类$C_k,k=1,2,...,K$，$|C_k|$为属于类$C_k$的样本个数，$\sum_{i=1}^K|C_k|=|D|$。设特征A有n个不同的取值$\{a_1,a_2,...,a_n\}$，根据特征A的取值将D划分为n个子集$D_1,D_2,...,D_n$，$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n|D_i|=|D|$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i \cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。

2. **信息增益算法**：

   + **输入**：训练数据集D和特征A。

   + **输出**：特征A对训练数据集的信息增益$g(D,A)$。

   + + 计算数据集D的经验熵$H(D)$：
       $$
       H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2 \frac{|C_k|}{|D|}
       $$

     + 计算特征A对数据集的经验条件熵$H(D|A)$：
       $$
       H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik|}}{|D_i|}\log {\frac{|D_{ik}|}{|D_i|}}
       $$

     + 计算信息增益：
       $$
       g(D,A)=H(D)-H(D|A)
       $$



### (4)信息增益比

1. 以信息增益作为划分训练数据集的特征，存在**偏向于选择取值较多的特征**的问题。

2. 使用信息增益比(information gain ratio)可以对这一问题进行校正，这是特征选择的另一准则。

3. **信息增益比**：

   特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集关于特征A的熵$H_A(D)$之比，即
   $$
   g_R(D,A)=\frac{g(D,A)}{H_A(D)}
   $$
   其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log \frac{|D_i|}{|D|}$，n是特征A的取值个数。