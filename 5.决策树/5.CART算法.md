# CART算法

1. 分类与回归树(classificaition and regression tree)，简称CART，同样由特征选择、树的生成以及剪枝组成，既可以用于分类也可以用于回归。
2. + CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。
   + CART假设决策树是二叉树，内部节点特征的取值为"是"和"否"，左分支是取值为”是“的分支，右分支是取值为"否"的分支。
   + 这样决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。
3. CART算法由以下两步组成：
   + **决策树生成**：基于训练数据生成决策树，生成的决策树要尽量大。
   + **决策树剪枝**：用验证数据集对已生成的树进行剪枝并选择最优子树，这时使用损失函数最小作为剪枝的标准。



## 一、CART生成

决策树的生成就就是递归地构建二叉决策树的过程：

+ 对回归树：用平方误差最小化准则。
+ 对分类树：用基尼系数(Gini index)最小化准则。

### (一)回归树的生成

1. + 假设X与Y分别是输入和输出变量，并且Y是连续变量，给定训练数据集$D=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$。

   + 一棵回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元上的输出值。

   + 假设已经将输入空间划分为M个单元$R_1,R_2,...R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：
     $$
     f(x)=\sum_{m=1}^M c_mI(x\in R_m)
     $$

   + 当输入空间的划分确定时，可以用平方误差$\sum_{x_i \in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。

   + 单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值。

     即$\hat{c}_m=ave(y_i|x_i \in R_m)$。

2. + 采用启发式的方法对输入空间进行划分，选择第j个变量$x^{(j)}$和它的取的值$s$，作为被切分变量(splitting variable)和切分点(splitting point)，并定义两个区域：
     $$
     R_1(j,s)=\{x|x^{(j)}\le s\} 和  R_2(j,s)=\{x|x^{(j)}> s\}
     $$

   + 然后选择最优且分变量j和最优切分点s，即求解：
     $$
     \min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
     $$
     对固定输入变量j可以找到最优切分点s。

   + $$
     \hat{c_1}=ave(y_i|x_i \in R_1(j,s))和\hat{c_2}=ave(y_i|x_i\in R_2(j,s))
     $$

   + 遍历所有输入变量，找到最优的切分变量j，构成一个对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。**这样的回归树通常称为最小二乘回归树(least squares regression tree)。**



### (二)最小二乘回归树生成算法

1. **输入**：训练数据集D;

2. **输出**：回归树$f(x)$;

3. 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

   + 选择最优且分变量j与切分点s，求解
     $$
     \min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
     $$
     遍历变量j，对固定的切分变量j扫描切分点s，使上式达到最小值的$(j,s)$。

   + 用选定的对$(j,s)$划分区域并决定相应的输出值：
     $$
     R_1(j,s)=\{x|x^{(j)}\le s\},\quad R_2(j,s)=\{x|x^{(j)}>s\}\\
     \hat{c}_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i,x\in R_m,m=1,2
     $$

   + 继续对两个子区域调用步骤(1),(2)，直至满足停止条件

   + 将输入空间划分为M个区域$R_1,R_2,...,R_M$，生成决策树：
     $$
     f(x)=\sum_{m=1}^M \hat{c}_m I(x\in R_m)
     $$



### (三)分类树的生成

1. 分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

2. **基尼指数**：分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为
   $$
   Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
   $$
   对于二分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数是$Gini(p)=2p(1-p)$，对于给定的样本集合，其基尼指数为$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$，这里$C_k$是D中属于第k类的样本子集，K是类的个数。

3. 如果样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分，即
   $$
   D_1=\{(x,y)\in D|A(x)=a\}，D_2=D-D_1
   $$
   则在特征A的条件下，集合D的基尼指数定义为
   $$
   Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
   $$

4. 基尼指数$Gini(D)$表示集合D的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合D的不确定性。

   基尼指数越大，样本集合的不确定性越大，这一点和熵类似。



### (四)CART生成算法

1. **输入**：训练数据集D，停止计算条件;
2. **输出**：CART决策树。
3. 根据训练数据集，从根节点开始，递归地对每个节点进行以下操作，构建二叉决策树：
   + 设节点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取得每个值a，根据样本点对$A=a$的测试为"是"或"否"将D分割成$D_1$和$D_2$两部分，计算基尼指数。
   + 在所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征和最优切分点。依最优特征与最优切分点，从现节点生成两个子节点，将训练数据集依特征分配到两个子节点中去。
   + 对两个子节点递归调用(1), (2)，直至满足停止条件。
   + 生成CART决策树。



## 二、CART剪枝

CART剪枝算法由两步组成：

+ 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\{T_0,T_1,...,T_n\}$。
+ 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

### (一)剪枝、形成一个子树序列

1. 在剪枝的过程中，计算子树的损失函数：
   $$
   C_{\alpha}(T)=C(T)+\alpha|T|
   $$
   其中，T为任意子树， $C(T)$为对训练数据的预测误差(如基尼指数)，$|T|$为子树的叶子节点个数，$\alpha\ge 0$为参数，$C_{\alpha}(T)$为参数是$\alpha$时的子树T的整体损失。参数$\alpha$权衡训练数据的拟合程度和模型的复杂度。

2. 对固定的$\alpha$，一定存在是损失函数$C_{\alpha}(T)$最小的子树，将其表示为$T_{\alpha}$。$T_{\alpha}$在损失函数$C_{\alpha}(T)$最小的意义下是最优的。容易验证这样的最优子树是唯一的。

3. + 当$\alpha$大的时候，最优子树的$T_{\alpha}$偏小。
   + 当$\alpha$小的时候，最优子树的$T_{\alpha}$偏大。
   + 当$\alpha=0$时，整体树是最优的。
   + 当$\alpha\rightarrow \infty$时，根节点组成的单节点树是最优的。

4. 经证明：可以用递归的方法对树进行剪枝，将$\alpha$从小到大，$0=\alpha_0<\alpha_1 < ... < \alpha_n < \infty$，产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,...,n$，剪枝得到的子树序列对应着区间$\alpha\in[\alpha_i,\alpha_{i+1}),i=0,1,...,n$的最优子树序列$\{T_0,T_1,...,T_n\}$，序列中的子树是嵌套的。

   具体地，从整体树$T_0$开始剪枝，对$T_0$的任意内部节点t，以t为单节点的损失函数是$C_{\alpha}(t)=C(t)+\alpha$。

   以t为根节点的子树$T_t$的损失函数是
   $$
   C_{\alpha}(T_t)=C(T_t)+\alpha|T_t|
   $$

   + 当$\alpha=0$及$\alpha$充分小时，有不等式：$C_{\alpha}(T_t)<C_{\alpha}(t)$。
   + 当$\alpha$增大时，在某一$\alpha$有$C_{\alpha}(T_t)=C_{\alpha}(t)$。
   + 当$\alpha$再增大时，不等式反向。

   只要$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而t的节点少，因此t比$T_t$更可取，对$T_t$进行剪枝。

   为此，为$T_0$种每一个内部节点t，计算
   $$
   g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
   $$
   它表示**剪枝后整体损失函数减少的程度**。在$T_0$中减去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。

   如此剪枝下去，直至得到根节点。在这一过程中，不断地增加$\alpha$的值，产生新的区间。



###  (二)在剪枝得到的子树序列$T_0,T_1,..,T_n$中通过交叉验证选取最优子树$T_{\alpha}$

1. 利用独立的验证数据集，测试子树序列$T_0,T_1,...,T_n$中各棵子树的平方误差或者基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_1,T_2,...,T_n$都对应于一个参数$\alpha_1,\alpha_2,...\alpha_n$。所以，当子树$T_k$确定时，对应的$\alpha_k$也确定了，即得到最优决策树$T_{\alpha}$。



### (三)CART剪枝算法

1. **输入**：CART算法生成的决策树$T_0$。

2. **输出**：最优决策树$T_{\alpha}$。

3. + 设$k=0,T=T_0$。

   + 设$\alpha=\infty$。

   + 自下而上地对各内部节点t计算$C(T_t)，|T_t|$以及$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$，$\alpha=\min(\alpha,g(t))$。

     这里$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数。

   + 对$g(t)=\alpha$的内部节点t进行剪枝，**并对叶节点t以多数表决法决定其类**，得到树T。

   + 设$k=k+1,\alpha_k=\alpha,T_k=T$。

   + 如果$T_k$不是由根节点及两个叶节点构成的树，则回到步骤2,否则，令$T_k=T_n$。

   + 采用交叉验证法在子树序列$T_0,T_1,...T_n$中选取最优子树$T_{\alpha}$。