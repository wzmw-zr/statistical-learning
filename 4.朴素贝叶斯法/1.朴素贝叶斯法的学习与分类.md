# 朴素贝叶斯法的学习与分类

朴素贝叶斯法(naive Bayes)是基于**贝叶斯定理**与**特征条件独立假设**的分类方法。

对于给定的训练数据集，首先**基于特征条件独立假设学习输入输出的联合概率分布**; 然后基于此模型，**对给定的输入x，利用贝叶斯订立求出后验概率的最大输出y。**

## 一、基本方法

1. + 设输入空间$X\subseteq R^n$为n维向量的集合，输出空间为类标记集合$y=\{c_1,c_2,..,c_K\}$。

   + 输入为特征向量$x\in X$，输出为类标记(class label)$y\in Y$。

   + $x$是定义在输入空间X上的随机向量，y是定义在输出空间Y上的随机变量。$P(X,Y)$是X和Y的联合概率分布。

   + 训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$由$P(X,Y)$独立同分布产生，

2. + 朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。

   + 具体地，学习以下先验概率分布及条件概率分布：
     + 先验概率分布$P(Y=c_k),k=1,2,...K$。
     + 条件概率分布$P(X=x|y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K$。
   + 于是学习到联合概率分布$P(X,Y)$。

3. 条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，**假设$x^{(j)}$可取值有$S_j$个，$j=1,2,..,n$，Y可取值有K个，那么参数个数为$K\prod_{j=1}^nS_j$。**

4. **朴素贝叶斯法对条件概率分布作了条件独立性的假设**，这是一个较强的假设。具体的，条件独立性假设是：
   $$
   P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\
   =\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
   $$

5. **朴素贝叶斯法**实际上**学习到的是生成数据的机制，因此属于生成模型**。条件独立假设等于是说用于分类的特征在确定的条件下都是条件独立的。

6. 朴素贝叶斯法分类时，**对给定的输入x，通过学习得到的模型计算后验概率分布为$P(Y=c_k|X=x)$，将后验概率最大的类作为x的类输出**。后验概率计算根据贝叶斯定理进行：
   $$
   P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}
   $$
   根据条件独立性假设：
   $$
   P(Y=c_k|X=x)=\frac{P(y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k}P(Y=c_k)\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)},\qquad k=1,2,...,K
   $$
   这是朴素贝叶斯法分类的基本公式。于是，**朴素贝叶斯法分类器**可表示为：
   $$
   y=f(x)=\arg \max_{c_k}\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}
   $$
   由于分母是定值，那么实际上是
   $$
   y=\arg \max_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
   $$



## 二、后验概率最大化的含义

1. 朴素贝叶斯法**将实例分到后验概率最大的类中。这等价于期望风险最小化。**

2. 假设分类决策函数$f(X)$，损失函数是$L(Y,f(X))$:

   + 期望风险函数为$R_{exp}(f)=E[L(Y,f(X))]$。

   + 期望是对联合分布的$P(X,Y)$取的，因此取条件期望
     $$
     R_{exp}(f)=E_X \sum_{k=1}^{K}[L(c_k,f(X))]P(c_k|X)
     $$
     为了使期望风险最小化，只需要对$X=x$逐个极小化，由此得到：
     $$
     f(x)=\arg \min_{y\in Y}\sum_{k=1}^K L(c_k,y)P(c_k|X=x)
     $$
     如果损失函数取0-1损失函数，则：
     $$
     f(x)=\arg \min_{y \in Y}\sum_{k=1}^K P(y \neq c_k|X=x)\\
     =\arg \min_{y \in Y}(1-P(Y=c_k|X=x))\\
     =\arg \max_{y\in Y}P(y=c_k|X=x)
     $$
     根据期望风险最小化准则就得到了后验概率最大化准则：
     $$
     f(x)=\arg \max_{c_k}P(c_k|X=x)
     $$
     

   