# 朴素贝叶斯法的参数估计

## 一、极大似然估计

1. 在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$。可以应用极大似然估计法估计相应的概率。

2. 先验概率$P(Y=c_k)$的极大似然估计是：
   $$
   P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},k=1,2,...,K
   $$
   设第j个特征$x^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},...,a_{jS_j}\}$，条件概率$P(X^{(j)}=a_j|Y=c_k)$的极大似然估计是：
   $$
   P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}\\
   j=1,2,..,n; \; l=1,2,...,S_j; \; k=1,2,...,K
   $$

其中，$x_i^{(j)}$是第i个样本的第j个特征，$a_{jl}$是第j个特征可能取的第l个值，I为指示函数。



## 二、学习与分类算法

1. **朴素贝叶斯算法(naive Bayes algorithm)**：

   + **输入**：训练数据$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$，$x_i^{(j)}$是第i个样本的第j个特征，$x_i^{(j)}\in \{a_{j1}.a_{j2},...a_{jS_j}\}$，$a_{jl}$是第j个特征可能取的第l个值，$j=1,2,..,n; \; l=1,2,..,S_j; \; y_i \in \{c_1,c_2,...,c_K\}$; 实例x。

   + **输出**：实例x的分类。

   + + 计算先验概率及条件概率：
       $$
       P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N}, \qquad k=1,2,..K\\
       P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N} I(y_i=c_k)}\\
       j=1,2,..,n; \quad l = 1,2,..,S_j; \quad k=1,2,..,K
       $$

     + 对于给定的实例$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算
       $$
       P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,...,K
       $$

     + 确定实例x的类：
       $$
       y=\arg \max_{c_k}P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
       $$
       



## 三、贝叶斯估计

1. 用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，是分类产生误差。

2. 具体的条件概率的贝叶斯估计：
   $$
   P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^B I(x^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)}+S_j,\lambda
   $$
   式中$\lambda\ge0$。等价于在随机变量各个取值的频数上赋予一个正数$\lambda>0$。当$\lambda=0$时就是极大似然估计。

    常取$\lambda=1$，这时称为拉普拉斯平滑(Laplacian smoothing)。

   显然，对任何$l=1,2,..S_j,k=1,2,..,K$，有
   $$
   P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)>0\\
   \sum_{l=1}^{S_j}P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=1
   $$
   同样，先验概率的贝叶斯估计是：
   $$
   P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)+\lambda}{N+K\lambda}
   $$
   

