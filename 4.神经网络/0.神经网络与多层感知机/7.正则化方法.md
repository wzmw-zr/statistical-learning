# 正则化方法(Regularization)

Regularization：减小方差的策略，即减小过拟合的策略。

误差可分解为：偏差，方差与噪声之和，即：误差=偏差+方差+噪声。

偏差度量了学习方法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。

方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。

噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。



L1 Regularization Term : $\sum\limits_{i=1}^N|w_i|$。

L2 Regularization Term:$\sum\limits_{i=1}^Nw_i^2$。(Also called weight decay)

> 理论证明：Deep Learning Chapter 7.



Dropout：随机失活。

优点：避免过度依赖某个神经元，实现减轻过拟合。

随机：dropout probability (eg. p = 0.5)

失活：weight = 0;

> 需要注意训练和测试两个阶段的数据尺度变化，测试时，神经元输出值需要乘以p。

其他正则化方法：

+ Batch normalization
+ Layer Normalization
+ Instance Normalization
+ Group Normalization

