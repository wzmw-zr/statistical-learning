# 正则化方法(Regularization)

Regularization：**减小方差**的策略，即**减小过拟合**的策略。

## 一、误差、偏差、方差、噪声

1. **误差**可分解为：偏差，方差与噪声之和，即：**误差=偏差+方差+噪声**。

2. **偏差**度量了学习方法的**期望预测与真实结果的偏离程度**，即刻画了学习算法本身的拟合能力。

3. **方差**度量了同样大小的训练集的**变动**所导致的学习**性能的变化**，即刻画了数据扰动所造成的影响。

4. **噪声**则表达了在当前任务上任何学习算法所能达到的**期望泛化误差的下界**。
5. **过拟合**：方差过大，在训练集表现良好，在测试集表现糟糕。



## 二、常见的正则项

常见的正则项有：L1 Regularization Term, L2 Regularization Term(Weight Decay)， Dropout，Batch Normalization, Layer Normalization, INstance Normalization, Group Normalization等。

### 1.L1 Regularization Term

L1 Regularization Term : $\sum\limits_{i=1}^N|w_i|$。

### 2.L2 Regularization Term

L2 Regularization Term:$\sum\limits_{i=1}^Nw_i^2$。(Also called weight decay)

> 理论证明：Deep Learning Chapter 7.(从等高线理解)

> 加入L1, L2正则项之后，实际上每次除了让Loss，Cost值较小，还需要考虑让正则化项尽可能小。

加入L2正则化项之后，目标函数为：
$$
Obj = Cost + \alpha\frac{\lambda}{2}\sum_{i=1}^Nw_i^2
$$
那么：
$$
w_{i+1}=w_i-\frac{\partial Obj}{\partial w_i}=w_i-(\frac{\partial Cost}{\partial w_i}+\alpha\lambda w_i)=(1-\alpha\lambda)w_i-\frac{\partial Cost}{\partial w_i}
$$
权重比没有正则化项时小，所以被称为Weight Decay。

### 3.Dropout

1. **Dropout**：随机失活。

2. **优点**：避免过度依赖某个神经元，实现减轻过拟合。

3. **随机**：dropout probability (eg. p = 0.5)

4. **失活**：weight = 0;

   > 需要注意训练和测试两个阶段的**==数据尺度变化==**，测试时，神经元输出值需要乘以p。
   >
   > 因为训练时采用了Dropout，但是测试时没有采用Dropout，所以测试时神经元输出值需要乘以p。

其他正则化方法：

+ Batch normalization
+ Layer Normalization
+ Instance Normalization
+ Group Normalization

