# 损失函数(Loss Function)

损失函数：衡量模型输出与真实的标签之间的差距。

损失函数(Loss Function): 损失函数是单样本与真实标签的差距。

代价函数(Cost Function): 整体样本的损失函数的平均值。

目标函数(Objective Function)：
$$
Obj=Cost+Regularzation \;Term
$$
常用的损失函数：

1. MSE(均方误差，Mean Squared Error)

   输出与标签之间的平方的均值，常在回归任务中使用。

2. CE(交叉熵，Cross Entropy)

   交叉熵源自信息论，用于衡量两个分布的差异，常用于分类任务中。
   $$
   H(p,q)=-\sum_{i=1}^np(x_i)\log(q(x_i))
   $$
   

信息熵：描述信息的不确定度。

自信息：$I(x)=-\log(P(x)),P(x)$是某事件发生的概率。

信息熵 = 所有可能取值的信息量的期望，即：
$$
H(x)=-\sum_{i=1}^Np_i\log(p_i)
$$
信息熵越大，表示数据的不确定性越大，否则，数据确定性越高。



相对熵：又称K-L散度，衡量两个分布之间的差异:
$$
D_{KL}(P||Q)=\sum_{i=1}^NP(x_i)(\log{P(X_i)-\log{Q(x_i)}})
$$

$$
H(p,q)=H(P)+D_{KL}(P||Q)
$$

> 交叉熵=信息熵+相对熵。因此，优化交叉熵就相当于优化相对熵，因为信息熵是常数。

概率具有$0\le P(x)\le 1$，且概率和等于1的性质，但是在矩阵乘法下可能会出现负数或概率和大于1,那么就需要使用softmax函数，将数据变换到符合概率分布的形式。

Softmax操作：

1. 取指数，实现非负。
2. 除以指数之和，实现和为1。



> 损失函数的设计会涉及算法类型、求导是否容易、数据中异常值的分布等问题。

