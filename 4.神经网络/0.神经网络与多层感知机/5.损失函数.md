# 损失函数(Loss Function)

损失函数：衡量模型输出与真实的标签之间的差距。

## 一、损失函数

1. **损失函数**(Loss Function): **单样本**与真实标签的差距。

2. **代价函数**(Cost Function): **整体样本**的损失函数的**平均值**。
   $$
   Cost = \frac{1}{N}\sum_{i=1}^Nf(\hat{y_i},y_i)
   $$

3. **目标函数**(Objective Function)：代价函数加正则化项。

$$
Obj=Cost+Regularzation \;Term
$$


## 二、常用的损失函数

常用的损失函数：均方误差(MSE)，交叉熵(CE)。

### 1.均方误差(Mean Squared Error, MSE)

**均方误差**(Mean Squared Error，MSE)： 输出与标签之间的差的平方的均值，常在**回归任务**中使用。
$$
MSE=\frac{\sum\limits_{i=1}^n(y_i-y_i^p)^2}{n}
$$

### 2.交叉熵(Cross Entropy, CE)

1. **交叉熵**(Cross Entropy，CE)： 交叉熵源自信息论，用于**衡量两个分布的差异**，常用于**分类任务**中。
   $$
   H(p,q)=-\sum_{i=1}^np(x_i)\log(q(x_i))
   $$
   这里$p$是真实的分布，$q$是模型得到的分布，使用交叉熵进行优化目的在于尽可能**减小模型的分布与真实的分布之间的差异，让model的分布逼近真实的分布。**

2. **自信息**：描述某一个事件的**信息量**，$I(x)=-\log(P(x)),P(x)$是某事件发生的概率。

3. **信息熵**：描述信息的**不确定度**，所有可能取值的**信息量的期望**，即：
   $$
   H(x)=-\sum_{i=1}^Np_i\log(p_i)
   $$
   **信息熵越大，表示数据的不确定性越大**，否则，数据确定性越高。

4. **相对熵**：又称**K-L散度**，**衡量两个分布之间的差异**:
   $$
   D_{KL}(P||Q)=\sum_{i=1}^NP(x_i)(\log{P(X_i)-\log{Q(x_i)}})
   $$
   其中，P是真实分布，Q是model的分布。

   交叉熵、信息熵与相对熵之间的关系：

$$
H(p,q)=H(P)+D_{KL}(P||Q)
$$

> 交叉熵=信息熵+相对熵。因此，**==优化交叉熵就相当于优化相对熵==**，因为信息熵是常数。

5. 交叉熵要求预测是概率分布的形式，但是预测不一定是概率分布的形式：

   + 概率具有$0\le P(x)\le 1$，但是出现负数。

   + 概率和等于1，但是出现概率和不等于1。

     那么就需要**使用softmax函数，将数据变换到符合概率分布的形式。**

6. Softmax操作：

   + 取指数，实现非负。

   + 除以指数之和，实现和为1。
     $$
     y_i=S(z_i)=\frac{e^{z_i}}{\sum\limits_{j=1}^ne^{z_j}}
     $$



> 还有很多损失函数，可以去机器学习库看，不过**==损失函数的设计会涉及算法类型、求导是否容易、数据中异常值的分布等问题。==**

