# 统计学习的分类

## 一、基本分类(basic classification)

统计学习或者机器学习一般包括监督学习，无监督学习，强化学习。



### (一)监督学习(supervised learning)

监督学习是指从标注数据中学习预测模型的机器学习问题。**标注数据表示输入输出的对应关系**，预测模型对给定的输入产生相应的输出。**监督学习的本质是==学习输入到输出的映射的统计规律==**。

#### (1)输入空间、特征空间和输出空间

1. **输入空间**：输入的所有可能取值的集合。

2. **输出空间**：输出的所有可能取值的集合。

3. 每一个具体的输入是一个**实例**，通常由**特征向量(feature vector)**表示。所有特征向量存在的空间称为**特征空间**。特征空间的**每一个维度对应一个特征。**

4. 模型实际上都是定义在特征空间上的。

5. 在监督学习中，将**输入和输出看作是定义在输入空间与输出空间上的随机变量的取值。**

   > 一般变量用大写字母表示，而变量的取值用小写字母表示。
   >
   > 例如，输入实例$x$的特征向量记作：
   > $$
   > x=(x^{(1)},x^{(2)},..,x^{(n)})^{T}
   > $$
   > 其中$x^{(i)}$表示的是$x$的第i个特征，而$x_i$表示的是第i个变量。

6. 监督学习从训练数据集合中学习模型，对测试数据进行预测。训练数据由输入(或特征向量)与输出对组成，训练集通常表示为
   $$
   T=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}
   $$
   测试数据也由输入与输出对组成。输入与输出对又称样本或样本点。

7. 根据输入变量X和输出变量Y的类型，给予预测任务不同的名称：

   + **输入变量和输出变量都是连续变量**的预测问题称为**回归问题**。
   + **输出变量为有限个离散变量**的预测问题称为**分类问题**。
   + **输入变量与输出变量均为变量序列**的预测问题称为**标注问题**。



#### (2)联合概率分布

1. 监督学习假设**输入与输出的随机变量X和Y遵循联合概率分布$P(X,Y)$。**$P(X,Y)$表示概率分布函数，或者概率密度函数。

2. 统计学习假设数据存在一定的统计规律，**X和Y具有联合概率分布就是监督学习关于数据的基本假设**。

> 在学习过程中，假定这一联合概率分布存在，但是对于学习系统来说，联合概率分布的具体定义是未知的。
>
> 训练数据与测试数据被看作是依联合概率分布$P(X,Y)$独立同分布产生的。



#### (3)假设空间(hypothesis space)

1. 监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。即，监督学习的目的就在于找到最好的这样的模型。

2. **模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。**

3. 监督学习的模型可以是**概率模型或非概率模型**，由**条件概率分布$P(Y|X)$或决策函数$Y=F(X)$表示**。对具体的输入进行相应的输出预测时，写作$P(y|x)$或$y=f(x)$。



#### (4)问题的形式化

1. 监督学习分为学习和预测两个过程，由学习系统和预测系统完成。
2. 在学习过程中，学习系统利用给定的训练数据集，通过学习得到一个模型，表示为条件概率分布$P(Y|X)$或者决策函数$Y=F(X)$。条件概率分布$P(Y|X)$和决策函数$Y=F(X)$描述输入与输出随机变量之间的映射关系。
3. 在监督学习中，假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的。



### (二)无监督学习

1. + 无监督学习是指**从无标注数据中学习预测模型**的机器学习问题。

   + 无标注数据是自然得到的数据，预测模型表示数据的类别，转换或者概率。

   + **无监督学习的本质是学习数据中的统计规律或潜在结构。**

2. + 模型的输入与输出的所有可能取值的集合分别称之为输入空间与输出空间。
   + 每个输入是一个实例，由特征向量表示。
   + 每一个输出是对输入的分析结果，由输入的类别，转换或者概率表示。
   + 模型可以实现**对数据的聚类、降维或概率估计。**

3. + 假设$\chi$是输入空间，$Z$是隐式结构空间。
   + 要学习的模型可以表示为决策函数$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式，其中$x\in \chi$是输入，$z\in Z$是输出。
   + 包含所有可能模型的集合称为假设空间。
   + 无监督学习旨在从假设空间中选出在给定评价标准下的最优模型。

4. 无监督学习通常使用大量的无标注数据学习，每一个样本都是一个实例。训练数据表示为$U=\{x_1,x_2,..,x_N\}$，其中$x_i,i=1,2,...,N$是样本。

5. 无监督学习也是由学习系统和预测系统组成。



### (三)强化学习(reinforcement learning)

1. + 强化学习是指**智能系统在与环境的连续互动中学习最优行为策略**的机器学习问题。
   + 通常假设智能系统与环境的互动是基于马尔可夫决策过程(Markov decision process)，智能系统观测到的是与环境互动得到的数据序列。
   + 强化学习的本质是**学习最优的决策序列。**

2. 强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$<S,A,P,r,\gamma>$组成：

   + $S$是有限状态集合

   + $A$是有限动作集合

   + $P$是状态转移概率(transition probability)函数：
     $$
     P(s^{'}|s,a)=P(s_{t+1}=s^{'}|s_t=s,a_t=a)
     $$

   + $r$是奖励函数：$r(s,a)=E(r_{t+1}|s_t=s,a_t=a)$

   + $\gamma$是衰减系数：$\gamma \in [0,1]$

3. 马尔可夫决策过程具有马尔可夫性，**下一个状态只依赖于前一个状态与动作**，由状态转移概率函数$P(s^{'}|s,a)$表示。**下一个奖励依赖于前一个状态与动作**，由奖励函数$r(s,a)$表示。

4. **价值函数(value function)或者状态价值函数(state value function)**定义为**策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望**：
   $$
   v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...|s_t=s]
   $$

5. **动作价值函数(action value function)**定义为策略$\pi$的从某一个状态$s$和某一个动作$a$开始的长期累计奖励的数学期望：
   $$
   q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...|s_t=s,a_t=a]
   $$

6. 强化学习的目标就是**在所有可能的策略中选出价值函数最大得策略$\pi^*$，**而在实际学习中往往从具体的策略出发，不断优化已有的策略，这里的$\gamma$表示未来的奖励会有所衰减。

7. 强化学习方法分类：

   + **无模型的方法：**

     + 无模型、**基于策略的方法**(policy-based)不直接学习模型，而是试图**求解最优策略$\pi^*$，表示为决策函数$a=f^*(s)$或者是条件概率分布$P^*(a|s)$**，这样也可以达到在环境中做出最优决策的目的。**学习通常从一个具体的策略开始，通过搜索更优的策略进行。**
     + 无模型、**基于价值的方法**(value-based)也不直接学习模型，而是试图**求解最优价值函数**，特别是**最优动作价值函数$q^*(s,a)$。**这样可以间接地学到最优策略，根据该策略在给定的状态下做出相应的动作。学习通常**从一个具体的价值函数开始，通过搜索更优的价值函数进行。**

   + **有模型的方法：**

     有模型的方法试图**直接学习马尔可夫决策过程的模型**，包括**状态转移概率函数$P(s^{'}|s,a)$和奖励函数$r(s,a)$。**这样可以通过模型对环境的反馈进行预测，求得价值函数最大的策略$\pi^*$。



### (四)半监督学习与主动学习

1. 半监督学习(semi-supervised learning)是指利用标注数据和未标注数据学习预测模型的机器学习问题。
2. 主动学习(active learning)是指机器不断主动给出实例给人进行标注，然后利用标注数据学习预测模型的机器学习问题。



##  二、按模型分类

### (一)概率模型与非概率模型

1. 统计学习的模型可以分为**概率模型**(probability model)和**非概率模型**(non-probability model)或者**确定性模型**(deterministic model)。

2. + 在监督学习中，概率模型取条件概率分布形式$P(y|x)$，非概率模型取函数形式$y=f(x)$，其中$x$是输入，$y$是输出。
   + 在无监督学习中，概率模型取条件概率分布形式$P(z|x)$或$P(x|z)$，非概率模型取函数形式$z=g(x)$，其中x是输入，z是输出。

3. + 概率模型：决策树，朴素贝叶斯，隐马尔可夫模型，条件随机场，概率潜在语义分析，潜在狄利克雷分配，高斯混合模型等。
   + 非概率模型：感知机，支持向量机，k近邻，AdaBoost，k均值，潜在语义分析，神经网络等。

   > Logistic regression既可以看作是概率模型，也可以看作是非概率模型。

4. 条件概率分布$P(y|x)$和函数$y=f(x)$可以相互转化(条件概率分布$P(y|x)$和函数$z=g(x)$同样可以)。

   + **条件概率分布最大化之后得到函数，函数归一化之后得到条件概率分布。**
   + **概率模型和非概率模型的区别**不在于输入与输出之间的映射关系，而在于**模型的内在结构**。
   + 概率模型通常可以表示为联合概率分布的形式，其中的变量表示输入、输出、隐变量甚至参数。而非概率模型则不一定存在这样的联合概率分布。

5. 概率模型的代表是**概率图模型**(probabilistic graphical model)，概率图模型是**联合概率分布由有向图或者无向图表示的概率模型**，而**联合概率分布可以根据图的结构分解为因子乘积的形式**。

   > 贝叶斯网络、马尔可夫随机场、条件随机场是概率图模型。

   概率图模型中常用的概率公式：
   $$
   P(x)=\sum_{y}P(x,y)，\qquad 贝叶斯公式，这里也称加法规则\\
   P(x,y)=P(x)P(y|x)，\qquad 条件概率公式，这里也称乘法规则
   $$



### (二)线性模型与非线性模型

1. 统计学习模型，特别是非概率模型，可以分为线性模型(linear model)和非概率模型(non-linear model)。如果函数$y=f(x)$或$z=g(x)$是线性函数，则称模型是线性模型，否则称模型是非线性模型。

2. + 线性模型：感知机，线性支持向量机，k近邻，k均值，潜在语义分析等是线性模型。

   + 非线性模型：支持向量机，AdaBoost，神经网络等是非线性模型。

     > 深度学习实际是复杂神经网络的学习，也就是复杂的非线性模型的学习。



### (三)参数化模型与非参数化模型

1. + 参数化模型(parametric model)：假设**模型参数的维数固定**，模型可以由有限0维参数完全刻画。
   + 非参数化模型(non-parametric model)：假设**模型参数的维数不固定或者说无穷大**，随着训练数据量的增加而不断增大。
2. + 参数化模型：感知机，朴素贝叶斯，Logistic regression，k均值，高斯混合模型，潜在语义分析，概率潜在语义分析，潜在狄利克雷分配等。
   + 非参数化模型：决策树，支持向量机，AdaBoost，k近邻等。



##  三、按算法分类

1. 统计学习根据算法，可以分为在线学习(online learning)与批量学习(batch learning)。
   + 在线学习：**每次接受一个样本，进行预测，之后学习模型**，并不断重复该操作的机器学习。
   + 批量学习：**一次接受所有数据，学习模型，之后进行预测**。



## 四、按技巧分类

### (一)贝叶斯学习

1. 贝叶斯学习(Bayesian learning)，又称贝叶斯推理(Bayesian inference)。

2. 其主要思想：在概率模型的学习和推理中，利用贝叶斯定理，**计算在给定数据条件下模型的条件概率，即后验概率**，并利用这个原理进行模型的估计，以及对数据的预测。

3. **将模型、未观测要素及其参数用变量表示，使用模型的先验分布**是贝叶斯学习的特点。

4. 假设随机变量D表示数据，随机变量$\theta$表示模型参数。根据贝叶斯定理，可以使用以下公式计算后验概率$P(\theta | D)$：
   $$
   P(\theta | D)=\frac{P(\theta)P(D|\theta)}{p(D)}
   $$
   
   其中$P(\theta)$是先验概率，$P(D|\theta)$是似然函数。

> 关于先验概率与后验概率：
>
> + 先验概率：事情还没有发生，根据以往的经验来判断事情发生的概率。是“由因求果”的体现。
> + 后验概率：事情已经发生了，有多种原因，判断事情的发生是由哪种原因引起的，是“由果求因”。

5. 模型估计时，估计整个后验概率分布$P(\theta|D)$。如果需要给出一个模型，通常取后验概率最大的模型。

6. 预测时，计算数据对后验概率分布的期望值：
   $$
   P(x|D)=\int P(x|\theta,D)P(\theta | D)d\theta
   $$



### (二)核方法

1. 核方法(kernel method)是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和非监督学习。
2. 向量内积运算等基于相似度计算的线性模型的学习方法，核方法可以将它们扩展到非线性模型的学习。
3. 核函数支持向量机、核PCA、核k均值等均属于核方法。
4. 把线性模型扩展到非线性模型：
   + 直接的做法是显式地定义从输入空间到特征空间的映射，在特征空间中进行内积计算。
   + 核方法的技巧在于不显式定义这个映射，而是直接定义核函数，即映射之后在特征空间的内积。