# 两类机器学习理论：频率派与贝叶斯派

机器学习引入了概率，根据方法与理论可以将机器学习算法大致分成频率派和贝叶斯派。

> **==严格来说，是概率论分成频率派和贝叶斯派。==**

一般而言，机器学习的问题可以简化为如下形式问题：

+ 训练数据集$X=T=(x_1,x_2,...,x_N)^T$，其中$x_i=(x_{i1},x_{i2},...,x_{ip})$。
+ 参数$\theta$。
+ 概率模型：$x\sim \mathcal{p}(x|\theta)$。

## 一、频率派

频率派认为**模型的参数$\theta$是未知的常量**，$x$是数据(即变量)，为了估计出常量$\theta$，最常用的就是**极大似然估计(MLE)**：
$$
\theta_{MLE}=\arg\max_{\theta}\log p(x;\theta)
$$
> 这里$p(x;\theta)$中用的是分号，即表示$\theta$是一个常量，一般我们有的是完全未知的概率密度$p(x|w_i)$，为了强调概率分布与$\theta_i$有关，我们将对应的概率密度记作$p(x|w_i;\theta_i)$，分号的意思同理。

这里使用$\log$是因为通常认为数据是独立同分布的，即：
$$
x_i \mathop{\sim}^{iid}p(x_i;\theta)
$$
一般设:
$$
p(x;\theta)=\prod_{i=1}^Np(x_i;\theta)
$$
那么使用$\log$就可以将乘法转成加法。

>  这里的极大似然估计对应于一个**类条件概率密度函数**。
>
> 极大似然估计具有渐进无偏和渐进一致性。
>
> MLE求的是怎样的参数$\theta$可以让事件集发生的概率最大。通过不断改变固定的参数$\theta$去寻找一个极大值。最大似然估计是$\theta$的函数，其求解过程就是找到使得似然函数最大的那个参数$\theta$。

**==频率派发展出来统计机器学习，需要解决的问题实际上就是一个优化问题==，比如给出模型和Loss Function，确定算法。**



## 二、贝叶斯派

贝叶斯派认为**模型的参数$\theta$是一个随机变量，服从一定的概率分布**。

这个概率分布为$\theta\sim \phi(\theta)$，一般称$\phi(\theta)$为先验概率。

**借助贝叶斯定理，将参数的先验概率和后验概率，通过似然函数联系起来。**

> 因为后验概率本来就是通过贝叶斯公式，用先验概率和似然函数计算出来的。
>
> 常理而言，事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。
>
> 但是，如果形式化来说，对于随机变量$x，\theta$，随机变量的概率密度为$p(x|\theta)$，先验概率密度是$p(\theta)$，后验概率密度是$p(\theta|x)$  [这都是连续随机变量，离散的称为概率分布]。

贝叶斯公式：
$$
p(\theta|X)=\frac{p(X|\theta)p(\theta)}{p(X)}=\frac{p(X|\theta)p(\theta)}{\int_{\theta}p(X|\theta)p(\theta)\mathrm{d}\theta}
$$
在最大后验估计(Maximum a posterior, MAP)中，$\theta$被看作是一个随机变量。在最大后验估计中，$\theta$是有概率意义的，$\theta$有自己的分布，而这个分布函数，需要通过已有的样本集合X得到。而贝叶斯公式就是参数$\theta$关于已有数据集合X的后验概率，要使得这个后验概率最大，也需要对后验概率函数求导，由于$p(X)$相对$\theta$是独立的，所以可以直接忽略掉$p(X)$。
$$
\theta_{MAP}=\arg\max_{\theta}p(\theta|X)=\arg\max_{\theta}p(\theta)p(X|\theta)
$$
对于$\theta$求导，
$$
\frac{p(\theta|X)}{\partial\theta}=\frac{p(\theta)p(X|\theta)}{\partial\theta}=0
$$
这里$P(X|\theta)$和极大似然估计中的似然函数$p(X;\theta)$是一样的，只是记法不一样。

MAP和MLE的区别：MAP在MLE的基础上加上了$p(\theta)$。

> 虽然从公式上看，$MAP=MLE*p(\theta)$，但是这两种算法有本质的区别，MLE将$\theta$视为一个未知的常量，MAP则将$\theta$视为一个随机变量。
>
> **==MAP与MLE最大区别是MAP中加入了模型参数本身的概率分布，或者说。MLE中认为模型参数本身的概率的是均匀的，即该概率为一个固定值。==**

如果我们进行预测，即根据已有数据集对某一结果进行预测，即其发生的概率：

$p(\hat{x}|X)=\int_{\theta}p(\hat{x},\theta|X)\mathrm{d}\theta=\int_{\theta}p(\hat{x}|\theta)p(\theta|X)\mathrm{d}\theta$，这里将$x $和X的关系解构成$x$和$\theta$的关系和$\theta$和X的关系，也就是说我们要进行预测，就要求出后验概率$p(\theta|X)$。

完整的贝叶斯公式的分母需要在整个参数空间对$\theta$作积分，计算是相当复杂的。**因此，从贝叶斯公式发展出来了概率图模型，或者使用蒙特卡罗方法求数值积分。**

**==贝叶斯派发展出来，实际上就是一个求积分的问题。==**