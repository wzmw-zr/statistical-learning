# 提升方法AdaBoost方法

## 一、提升方法的基本思路

1. 提升(boosting)方法在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。

2. **强可学习(strongly learnable)**:在概率近似正确(probably approximately correct, PAC)学习的框架中，一个概念 ，如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的，。

   **弱可学习(weakly learnable)**:一个概念，如果存在一个多项式的学习算法能够学习它，学习正确率仅比随机猜想好，那么就称这个概念是弱可学习的。

   **在PAC学习的框架下，一个概念是强可学习的充要条件是这个概念是弱可学习的。**

3. 提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器(又称基本分类器)，然后组合这些弱分类器，构成一个强分类器。大多数提升方法都是改变训练数据的概率分布(训练数据的权值分布)正对不同的训练数据分布调用弱学习算法学习一系列弱分类器。

4. **如何改变训练数据的权值或者概率分布**：AdaBoost 提高那些前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。这样，分类问题就被一系列弱分类器“分而治之”。

5. **如何将弱分类器组合成强分类器**：AdaBoost 采取加权多数表决的方法，具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。



## 二、AdaBoost算法

1. **输入**：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，$x_i \in \mathcal{X}\subseteq R^n$，$y_i \in \mathcal{Y} = \{+1,-1\}$，弱学习算法。

2. **输出**：最终分类器$G(x)$。

3. **步骤**：

   (1)  初始化训练数据的权值分布
   $$
   D_1=(w_{1,1},...w_{1,i},w_{1,N}),w_{1,i}=\frac{1}{N},i=1,2,...N
   $$
   (2)对$m=1,2,..,M$：

   + 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x):\mathcal{X}\rightarrow \{+1,-1\}$。

   + 计算$G_m(x)$在训练数据集上的分类误差率：
     $$
     e_m=\sum_{i=1}^N P(G_m(x_i)\neq y_i)=\sum_{i=1}^N w_{m,i}I(G_m(x_i)\neq y_i)
     $$

   + 计算$G_m(x)$的系数：
     $$
     \alpha_m=\frac{1}{2}\log{\frac{1-e_m}{e_m}}
     $$
     当$e_m \le \frac{1}{2}$时，$\alpha_m\ge 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差越小的基本分类器在最终的分类器中的作用越大。

   + 更新训练数据集的权值分布：
     $$
     D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\
     w_{m+1,i}=\frac{w_{m,i}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N
     $$
     这里，$Z_m$是规范化因子：
     $$
     Z_m=\sum_{i=1}^N w_{m,i}\exp(-\alpha_my_iG_m(x_i))
     $$
     它使$D_{m+1}$成为一个概率分布，即确保$\sum_{i=1}^N w_{m+1,i}=1$。

   + 构建基本分类器的线性组合：
     $$
     f(x)=\sum_{m=1}^M \alpha_mG_m(x)
     $$
     得到最终分类器：
     $$
     G(x)=\mathrm{sign}(f(x))=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))
     $$
     