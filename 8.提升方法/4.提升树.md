# 提升树

提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。

## 一、提升树模型

1. 提升方法实际采用加法模型(即基函数的线性组合)与前向分步算法。以决决策树为基函数的提升方法称为提升树(boosting tree)。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。

2. 提升树模型可以表示为决策树的加法模型：
   $$
   f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
   $$
   其中，$T(x;\Theta_m)$表示决策树，$\Theta_m$为决策树的参数，M为树的个数。



## 二、提升树算法

1. 提升树算法采用前向分步算法。首先确定初始提升树$f_0(x)=0$，第m步的模型是：
   $$
   f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
   $$
   其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$:
   $$
   \hat{\Theta}_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
   $$

2. 由于数的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。

3. 不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。



## 三、梯度提升

1. 提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失函数和指数损失函数时，每一步的优化很简单，但是对于一般损失函数而言，每一步的优化并不是那么容易。

2. 针对一般损失函数下提升树优化困难，可以使用梯度提升(gradient boosting)。这是最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值：
   $$
   -[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
   $$
   作为回归问题提升树算法中的残差的近似值，拟合一个回归树。



## 四、梯度提升算法

1. **输入**：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\},x_i\in \mathcal{X}\subseteq R^n,y_i\in \mathcal{Y}\subseteq R$;损失函数$L(y,f(x))$。

2. **输出**：回归树$\hat{f}(x)$。

3. + 初始化
     $$
     f_0(x)=\arg\min_c\sum_{i=1}^NL(y_i,c)
     $$

   + 对$m=1,2,...M$

     + 对$i=1,2,...,N$，计算：
       $$
       r_{m,i}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
       $$

     + 对$r_{m,i}$拟合一个回归树，得到第m棵树的叶节点区域$R_{m，j},j=1,2,...,J$。

     + 对$j=1,2,...,J$，计算：
       $$
       c_{m,j}=\arg\min_c\sum_{x_i \in R_{m,j}}L(y_i,f_{m-1}(x_i)+c)
       $$

     + 更新$f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{m,j}I(x\in R_{m,j})$

   + 得到回归树：
     $$
     \hat{f}(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{m,j}I(x\in R_{m,j})
     $$
     