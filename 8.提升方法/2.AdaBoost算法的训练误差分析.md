# AdaBoost算法的训练误差分析

AdaBoost最基本的性质：能够在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。

## 一、AdaBoost的训练误差界

1. **定理**：AdaBoost算法最终分类器的训练误差界为：
   $$
   \frac{1}{N}\sum_{i=1}^NI(G(x_i)\neq y_i)\le \frac{1}{N}\sum_{i}\exp(-y_if(x_i))=\prod_{m}Z_m
   $$
   其中，$G(x)$为最终分类器，$f(x)$为基本分类器的线性组合，$Z_m$为规范化因子。
   
   > $$
   > \alpha_m=\frac{1}{2}\log{\frac{1-e_m}{e_m}}(e_m为G_m(x)在训练数据集上的分类误差率)\\
   > f(x)=\sum_{i=1}^m\alpha_m G_m(x)\\
   > G(x)=\mathrm{sign}(f(x))=\mathrm{sign}(\sum_{i=1}^M \alpha_m G_m(x))\\
   > Z_m=\sum_{i=1}^N w_{m,i}\exp(-\alpha_m y_i G_m(x_i))
   > $$
   
   > **证明**：
   >
   > 前半部分显然。
   >
   > 后半部分，因为：
   > $$
   > Z_m=\sum_{i=1}^Nw_{m,i}\exp(-\alpha_my_iG_m(x_i))\\
   > w_{m+1,i}=\frac{w_{m,i}\exp(-\alpha_my_iG_m(x))}{Z_m}\\
   > $$
   > 所以：
   > $$
   > \frac{1}{N}\sum_{i}\exp(-y_if(x_i))=\frac{1}{N}\sum_i\exp(-y_i\sum_{m=1}^M\alpha_mG_m(x_i))
   > $$
   > 因为$w_{1,i}=\frac{1}{N}$，所以：
   > $$
   > \begin{aligned}
   > \frac{1}{N}\sum_{i}\exp(-y_if(x_i))&=\sum_{i}w_{1,i}\prod_{m=1}^M\exp(-\alpha_my_iG_m(x_i))\\
   > &=Z_1\sum_{i}\frac{w_{1,i}}{Z_1}\prod_{m=1}^M\exp(-\alpha_my_iG_m(x_i))\\
   > &=Z_1\sum_{i}\frac{w_{1,i}\exp(-\alpha_1y_iG_1(x_i))}{Z_i}\prod_{m=2}^M\exp(-\alpha_my_iGm(x_i))\\
   > &=Z_1\sum_{i}w_{2,i}\prod_{m=2}^M\exp(-\alpha_m y_i G_m(x_i))\\
   > &=Z_1Z_2\sum_{i}w_{3,i}\prod_{m=3}^M\exp(-\alpha_my_iG_m(x_i))\\
   > &=...\\
   > &=Z_1Z_2....Z_{M-1}\sum_{i}w_{M,i}\exp(-\alpha_My_iG_M(x_i))\\
   > &=\prod_{m=1}^MZ_m
   > \end{aligned}
   > $$
   
   这一定理说明，在每一轮选取适当的$G_m $使得$Z_m$最小，可以使训练误差下降最快。



## 二、二类分类问题AdaBoost的训练误差界

1.**定理**：
$$
\prod_{m=1}^MZ_m=\prod_{m=1}^M[2\sqrt{e_m(1-e_m)}]\\
=\prod_{i=1}^M\sqrt{(1-4\gamma_m^2)}\\
\le \exp(-2\sum_{i=1}^M\gamma_m^2)\\
\gamma_m=\frac{1}{2}-e_m 
$$

> **证明**：
> $$
> \begin{aligned}
> Z_m&=\sum_{i=1}^Nw_{m,i}\exp(-\alpha_my_iG_m(x_i)))\\
> &=\sum_{y_i=G_m(x_i)}w_{m,i}e^{-\alpha_m}+\sum_{y_i\neq G_m(x_i)}w_{m,i}e^{\alpha_m}\\
> &=(1-e_m)e^{-\alpha_m}+e_me^{\alpha_m}\\
> &=(1-e_m)\exp({-\frac{1}{2}\log{\frac{1-e_m}{e_m}}})+e_m\exp(\frac{1}{2}\log{\frac{1-e_m}{e_m}})\\
> &=(1-e_m)\sqrt{\frac{e_m}{1-e_m}}+e_m\sqrt{\frac{1-e_m}{e_m}}\\
> &=2\sqrt{e_m(1-e_m)}\\
> &=\sqrt{1-4\gamma_m^2}
> \end{aligned}
> $$
>  根据$e^x$和$\sqrt{1-x}$在点$x=0$的泰勒展开式推出不等式$\sqrt{1-4\gamma_m^2}\le e^{-2\gamma_m^2}$。



2.推论：如果存在$\gamma>0$，对所有m有$\gamma_m\ge\gamma$，则：
$$
\frac{1}{N}\sum_{i=1}^NI(G(x_i)\neq y_i)\le \exp(-2M\gamma^2)
$$
这表明在此条件下AdaBoost的训练误差是以指数速率下降的。