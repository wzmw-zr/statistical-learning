# 最大熵模型(maximum entropy model)

## 一、最大熵原理

1. 最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，**熵最大的模型是最好的模型**。

   通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为**在满足约束条件的模型集合中选取熵最大的模型**。

2. 假设离散随机变量X的概率分布是$P(X)$，则其熵是：
   $$
   H(P)=-\sum_{x}P(x)\log P(x)
   $$
   熵满足下裂不等式：
   $$
   0 \le H(P) \le \log |X|
   $$
   式中，$|X|$是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。这就是说，**当X服从均匀分布时，熵最大**。

3. + 最大熵原理认为要选择的概率模型**首先必须满足约束条件**，**在没有更多信息的情况下，那些不确定的部分都是等可能的**。
   + 最大熵原理**通过熵的最大化来表示等可能性**。”等可能“不容易操作，而熵则似乎一个可优化的数值指标。

4. 最大上原理是统计学习的一般原理，把它应用到分类得到最大熵模型。



## 二、最大熵模型的定义

1. 假设分类模型是一个条件概率分布$P(Y|X)$，$X\in \mathcal{X} \subseteq R^n$表示输入，$Y \in \mathcal{Y}$表示输出，$\mathcal{X}$和$\mathcal{Y}$分别是输入和输出的集合。

   这个模型表示的是**对于给定的输入$X$，以条件概率$P(Y|X)$输出$Y$。**

2. 给定一个训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，学习的目标是**用最大熵原理选择最好的分类模型**。

   + 首先考虑模型应该满足的条件。给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布，分别以$\widetilde{P}(X,Y)$和$\widetilde{P}(X)$表示：
     $$
     \widetilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}\\
     \widetilde{P}(X=x)=\frac{v(X=x)}{N}
     $$
     其中，$v(X=x,Y=y)$表示训练数据中样本$(x,y)$出现的频数，$v(X=x)$表示训练数据输入x出现的频数，N表示训练样本的容量。

   + 用特征函数(feature function)$f(x,y)$描述输入x和输出y之间的某一个事实，其定义是：
     $$
     f(x,y)=
     \left\{
     \begin{array}{**lr**}
     1,\qquad x与y满足某一事实\\
     0, \qquad 否则
     \end{array}
     \right.
     $$
     这是一个二值函数，当x和y满足这个事实时取1,否则取0。

   + 特征函数$f(x,y)$关于经验分布$\widetilde{P}(X,Y)$的期望值，用$E_{\widetilde{P}}(f)$表示：
     $$
     E_{\widetilde{P}}(f)=\sum_{x,y}\widetilde{P}(x,y)f(x,y)
     $$

   + 特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\widetilde{P}(X)$的期望值，用$E_{P}(f)$表示：
     $$
     E_P(f)=\sum_{x,y}\widetilde{P}(x)P(y|x)f(x,y)
     $$

   + 如果模型能够获取训练数据中的信息，那么就可以假设这**两个期望值相等，即$E_{\widetilde{P}}(f)=E_P(f)$。**

     **将此作为模型学写的约束条件。**

   + 加入有n个特征函数$f_i(x,y),i=1,2,..,n$，那么就有n个约束条件。

3. **最大熵模型**：假设满足所有约束条件的模型集合为：
   $$
   \mathcal{C}\equiv\{P\in \mathcal{P}|E_P(f_i)=E_{\widetilde{P}}(f_i),i=1,2..,n\}
   $$
   定义在条件概率分布$P(Y|X)$上的条件熵为：
   $$
   H(P)=-\sum_{x,y}\widetilde{P}(x)P(y|x)\log P(y|x)
   $$
   则模型集合$\mathcal{C}$中条件熵$H(P)$最大的模型称为最大熵模型。



## 三、最大熵模型的学习

1. 最大熵模型的学习过程就是求解最大熵模型的过程。**最大熵模型的学习可以形式化为约束最优化问题。**

2. 对于给定的训练数据集$T=\{(x_1,y_1),(x_2,.y_2),...(x_N,y_N)\}$以及特征函数$f_i(x,y),i=1,2,..,n$，最大熵模型的学习等价于约束最优化问题：
   $$
   \max_{P\in C}H(P)=-\sum_{x,y}\widetilde{P}(x)P(y|x)\log P(y|x)\\
   s.t.\quad E_P(f_i)=E_{\widetilde{P}}(f_i),i=1,2,...n\\
   \sum_y P(y|x)=1
   $$
   按照最优化问题的习惯，将求最大值问题转化为等价的求最小值问题：
   $$
   \min_{P\in C}-H(P)=\sum_{x,y}\widetilde{P}(x)P(y|x)\log P(y|x)\\
   s.t.\quad E_P(f_i)=E_{\widetilde{P}}(f_i),i=1,2,...n\\
   \sum_y P(y|x)=1
   $$

3. 这里，将约束最优化的原始问题转换为无约束最优化的对偶问题。通过求解对偶问题求解原始问题。

   + 首先，引入拉格朗日乘子$w_0,w_1,w_2,...,w_n$，定义拉格朗日函数$L(P,w)$：
     $$
     L(P,w)=-H(P)+w_0(1-\sum_y P(y|x))+\sum_{i=1}^nw_i(E_{\widetilde{P}}(f_i)-E_P(f_i))\\
     =\sum_{x,y}\widetilde{P}(x)P(y|x)\log P(y|x)+w_0(1-\sum_y P(y|x))+\sum_{i=1}^nw_i(\sum_{x,y}\widetilde{P}(x,y)f_i(x,y)-\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y))
     $$

   + 优化的原始问题是：$\min_{P\in C} \max_w L(P,w)$;

     对偶问题是：$\max_{w} \min_{P\in C}L(P,w)$。

   + 由于拉格朗日函数$L(P,w)$是P的凸函数，原始问题的解与对偶问题的解是等价的。

4. + 首先，求解对偶问题的内部极小化问题$\min_{P\in C}L(P,w)$。$\min_{P\in C}L(P,w)$是$w$的函数，将其记作：
     $$
     \Psi(w)=\min_{P\in C}L(P,w)=L(P_w,w)
     $$
     $\Psi$称为对偶函数，同时，将其解记作:
     $$
     P_w=\arg \min_{P\in C}L(P,w)=P_w(y|x)
     $$

   + 具体地，求$L(P,w)$对$P(y|x)$的偏导数
     $$
     \frac{\partial L(P,w)}{\partial P(y|x)}=\sum_{x,y}\widetilde{P}(x)(\log P(y|x)+1)-\sum_y w_0-\sum_{x,y}(\widetilde{P}(x)\sum_{i=1}^nw_if_i(x,y))\\
     =\sum_{x,y}\widetilde{P}(x)(\log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y))
     $$
     令偏导数等于0,在$\widetilde{P}(x)>0$的情况下，解得：
     $$
     P(y|x)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}
     $$
     由于$\sum_{y}P(y|x)=1$，得：
     $$
     P_w(y|x)=\frac{1}{Z_w(x)}e^{(\sum_{i=1}^nw_if_i(x,y))}
     $$
     其中，$Z_w(x)=\sum_{y}e^{\sum_{i=1}^nw_if_i(x,y)}$。$Z_w(x)$被称为规范化因子，$f_i(x,y)$是特征函数，$w_i$是特征的权值。

   + 之后，求解对偶问题的外部极大化问题：$\max_w \Psi(w)$，将其解记为$w^*$，即$w^*=\arg \max_w \Psi(w)$。