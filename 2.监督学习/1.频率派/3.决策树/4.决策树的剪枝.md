# 决策树的剪枝

1. 决策树生成算法递归地产生决策树，直到不能继续下去位置。这样子产生的决策树通常会产生过拟合现象。

   产生过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。

   解决这个问题的方法是考虑决策树的复杂度，对已有的决策树进行简化。

2. 在决策树学习中将已生成的树进行简化的过程称为剪枝(pruning)。具体地，剪枝从已生成的树上裁掉一些子树或者叶节点，并将其根节点或父节点作为新的叶节点，从而简化决策树的模型。



## 一、决策树学习的剪枝算法

1. 决策树的剪枝往往通过**极小化决策树整体的损失函数(loss function)或代价函数(cost function)**来实现。

2. 设树T的叶节点个数为$|T|$，t是树T的叶节点，该叶节点有$N_t$个样本点，其中k类样本点有$N_{tk}$个，$k=1,2,..,K$，$H_t(T)$为叶节点t上的经验熵，$\alpha \ge 0$为参数，则决策树学习的损失函数可以定义为
   $$
   C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
   $$
   其中，经验熵为：
   $$
   H_t(T)=-\sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}
   $$
   在损失函数中，将$C_{\alpha}(T)$的第一项记作：
   $$
   C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk} \log \frac{N_{tk}}{N_t}
   $$
   此时有$C_{\alpha}(T)=C(T)+\alpha|T|$。

   式中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\alpha\ge 0$控制两者之间的影响。较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择较复杂的模型，$\alpha=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

3. 剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$\alpha$值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高，相反，子树越小，模型的复杂度就越低，但是往往与训练数据拟合地不好。损失函数恰好表达了对两者的平衡。

4. 决策树生成只考虑了通过提高信息增益或者信息增益比对训练数据进行更好的拟合。二决策树剪枝通过优化损失函数还考虑了减小模型复杂度。

   决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

5. **决策树剪枝的损失函数的极小化等价于正则化的极大似然估计。**

6. **决策树的剪枝算法**：

   + **输入**：生成算法产生的整棵树T，参数$\alpha$。
   + **输出**：剪枝后的子树$T_{\alpha}$。
   + + 计算每个节点的经验熵。
     + 递归地从树的叶节点向上回缩。设一组叶节点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_{\alpha}(T_B),C_{\alpha}(T_A)$，如果$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$，则进行剪枝，将父节点变为新的叶节点。
     + 返回上一步，直到不能继续位置，得到损失函数最小的子树$T_{\alpha}$。
   + 因为只需要考虑两棵树的损失函数的差，其计算可以在局部进行，所以，**决策树的剪枝算法可以由一种动态规划算法实现。**

