# 线性可分支持向量机与硬间隔最大化

## 一、线性可分支持向量机

1. 考虑一个二类分类问题。

   假设输入空间与特征空间为两个不同的空间。输入空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间。

   + **线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应**，并将输入空间中的输入映射为特征空间中的特征向量。
   + **非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。**

   所以，**输入都由输入空间转换到特征空间，==支持向量机的学习是在特征空间进行的==。**

2. 假设给定一个特征空间上的训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中，$x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。$x_i$为第i个特征响亮，也称为实例，$y_i$为$x_i$的类标记。

   当$y_i=+1$时，称$x_i$为正例，当$y_i=-1$时，称$x_i$为负例。$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。

   **学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类**。分离超平面对应于方程$w.x+b=0$，它由法向量$w$和截距$b$决定，可用$(w,b)$表示。分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类。**法向量指向的一侧为正类，另一侧为负类。**

3. 一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。

   + 感知机利用误分类最小的策略，求得分离超平面，不过这时解有无穷多个。
   + 线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。

4. **线性可分支持向量机**：给定线性可分训练数据集，通过间隔最大化或者等价地求解相应的凸二次规划问题学习得到的分离超平面为$w^*.x+b^*=0$，以及相应的分类决策函数$f(x)=\mathrm{sign}(w^*.x+b^*)$称为线性可分支持向量机。



## 二、函数间隔(functional margin)和几何间隔(geometric margin)

1. 一个点距离超平面的远近可以表示分类预测的确信程度。在超平面$w.x+b=0$确定的情况下，$|w.x+b|$能够相对地表示点x距离超平面的远近。而$w.x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可以用$y(w.x+b)$来表示分类准确性及确信度，这就是函数间隔。

2. **函数间隔**：对于给定的训练数据集$T$和超平面$(w,b)$，定义**超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为$\hat{\gamma_i}=y_i(w.x_i+b)$。**

   **定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔的最小值**，即$\hat{\gamma}=\min_{i=1,...,N}\hat{\gamma_i}$。

3. 函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w,b$，超平面并没有改变，但是函数间隔却变成了原来的2倍。因此，需要对分离超平面的法向量加约束，如规范化，$||w||=1$，使得间隔是确定的。这时的函数间隔成为几何间隔。

4. **几何间隔**：对于给定的训练数据集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为
   $$
   \gamma_i=y_i(\frac{w}{||w||}.x_i+\frac{b}{||w||})
   $$
   定义超平面$(w,b)$关于训练数据集T的几何间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的几何间隔的最小值，即$\gamma=\min_{i=1,2,...N}\gamma_i$。

5. 超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号距离(signed distance)，当样本点被超平面正确分类时就是实例点到超平面的距离。

6. 从函数间隔和几何间隔的定义可知，函数间隔和几何间隔有如下关系：
   $$
   \gamma_i=\frac{\hat{\gamma_i}}{||w||}\\
   \gamma=\frac{\hat{\gamma}}{||w||}
   $$
   



## 三、间隔最大化

1. 支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。

   对于线性可分的训练数据集而言，线性可分分离超平面有无穷多个，这与感知机相同，但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。

2. 间隔最大化：对训练数据集找到几何间隔最大超平面意味着以充分大的确信度对训练数据进行分类，即，**不仅将正负实例点分开，而且对最难分类的点(距离分离超平面最近的点)也有足够大的确信度将其分开。**

### (一)最大间隔分离超平面

求得集合间隔最大的分离超平面，可以表示为下面的约束最优化问题：
$$
\max_{w,b}\gamma\\
\mathrm{s.t.} \quad y_i(\frac{w}{||w||}.x_i+\frac{b}{||w||})\ge \gamma,\quad i=1,2,..,N
$$
考虑函数间隔与几何间隔的关系，问题等价于：
$$
\max_{w,b}\frac{\hat{\gamma}}{||w||}\\
\mathrm{s.t.} \quad y_i(w.x_i+b) \ge \hat{\gamma},\quad i=1,2,...,N
$$
因为函数间隔$\hat{\gamma}$的取值并不影响最优化问题的解，这样，就可以取$\hat{\gamma}=1$，将$\hat{\gamma}=1$代入上面的最优化问题，而此时的最优化问题就变成了$\max_{w,b}\frac{1}{||w||}$，相当于最小化分母$||w||$，为了求导方便，这就等价于最小化$\frac{1}{2}||w||^2$。因此，就得到下面线性可分支持向量机学习的最优化问题：
$$
\min_{w,b}\frac{1}{2}||w||^2\\
\mathrm{s.t.}\quad y_i(w.x_i+b)-1\ge 0，\quad i=1,2,...,N
$$
这是一个凸二次规划问题(convex quadratic programming)。

**线性可分支持向量机学习算法——最大间隔法(maximum margin method)**：

**输入**：线性可分训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}={+1,-1},i=1,2,...,N$。

**输出**：最大间隔分离超平面和分类决策函数。

**步骤**：

(1) 构造并求解约束最优化问题：
$$
\min_{w,b} \frac{1}{2}||w||^2\\
\mathrm{s.t.}\quad y_i(w.x_i+b)-1\ge0,\quad i=1,2,...,N
$$
求得最优解$w^*,b^*$。

(2)由此得到分离超平面：
$$
w^*.x_i+b^*=0
$$
分类决策函数：
$$
f(x)=\mathrm{sign}(w^*.x_i+b^*)
$$


### (二)最大间隔分离超平面的存在唯一性

1. **定理：(最大间隔分离超平面的存在唯一性)**：若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

   + **存在性**：因为训练数据集线性可分，所以根据感知机的证明，一定存在分离超平面。

   + **唯一性**：

     反证法证明最大间隔分离超平面的唯一性。

     先证明$w^*$的唯一性。

     假设存在两个最优解$(w_1^*,b_1^*),(w_2^*,b_2^*)$。由于最优化求得是$\min_{w,b}\frac{1}{2}||w||^2$，所以$||w_1^*||=||w_2^*||=C(C为常数)$。那么，令：
     $$
     w=\frac{w_1^*+w_2^*}{2}\\
     b=\frac{b_1^*+b_2^*}{2}
     $$
     将$(w,b)$代入约束条件，可以证明$(w,b)$也是一个最优解。辅以$L2$范数的三角不等式性质可以推导：
     $$
     C\le||w||=||\frac{w_1^*+w_2^*}{2}||\le\frac{||w_1^*||}{2}+\frac{||w_2^*||}{2}=C
     $$
     等号成立需要$w_1^*，w_2^*$是线性关系：$w_1^*=\lambda w_2^*$。

     又因为$||w_1^*||=||w_2^*||=C$， 则$\lambda=\pm1$，但是因为$w=\frac{1}{2}(w_1^*+w_2^*)$，因此$\lambda=1$。因此$w_1^*=w_2^*=w$，因此$w^*$的唯一性得到证明。

     

     下面证明$b^*$的唯一性。

     此时最优超平面可以写成$(w^*,b_1^*),(w^*,b_2^*)$。

     假设$x_1$是$(w^*,b_1^*)$对应集合$\{x_i|y_i=+1\}$中的点，那么$w^*.x_1+b_1^*-1=0$;

     假设$x_2$是$(w^*,b_2^*)$对应集合$\{x_i|y_i=+1\}$中的点，那么$w^*.x_2+b_2^*-1=0$;

     假设$x_3$是$(w^*,b_1^*)$对应集合$\{x_i|y_i=-1\}$中的点，那么$w^*.x_3+b_1^*+1=0$;

     假设$x_4$是$(w^*,b_2^*)$对应集合$\{x_i|y_i=-1\}$中的点，那么$w^*.x_4+b_2^*+1=0$;

     因此，$b_1^*=-\frac{1}{2}(w^*.x_1+w^*.x_3)$，$b_2^*=-\frac{1}{2}(w^*.x_2+w^*.x_4)$。

     则$b_1^*-b_2^*=-\frac{1}{2}[w^*.(x_1-x_2)+w^*.(x_3-x_4)]$。

     根据$y_i(w.x_i+b)-1\ge 0$的约束条件，将$x_2$代入到$(x^*,b_1^*)$对应超平面中，得到：
     $$
     w^*.x_2+b_1^*-1\ge 0
     $$
     同理，$w^*.x_1+b_2^*-1\ge 0$。

     因此，$w^*.x_1+b_2^*-1-(w^*.x_2+b_2^*-1)=w^*.x_1+b_2^*-1 -0 \ge 0$，即$w^*(x_1-x_2)\ge0$。

     同理可得$w^*.(x_2-x_1)\ge0$。

     因此，$x_1=x_2$。同理，$x_3=x_4$。因此$b_1^*-b_2^*=0$，即$b_1^*=b_2^*$。

     所以，假设的两个最优解$(w_1^*,b_1^*),(w_2^*,b_2^*)$是相同的，假设矛盾，不成立。

   + 综上，若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。



### (三)支持向量和间隔边界

1. **支持向量**：在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)。

   支持向量是使约束条件式等号成立的点，即$y_i(w.x_i+b)-1=0$。

2. 对于$y_i=+1$的正例点，支持向量在超平面$H_1:w.x+b=1$。

   对于$y_i=-1$的负例点，支持向量在超平面$H_2:w.x+b=-1$。

3. 注意到$H_1$和$H_2$平行，且没有实例点落在它们中间，在$H_1$和$H_2$之间形成了一条长带，长带的宽度，即$H_1$和$H_2$之间的距离称为间隔(margin)。间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$。$H_1,H_2$统称**间隔边界**。

4. **在决定分离超平面时只有支持向量起作用**，而其它实例点并不起作用。如果移动支持向量就会改变所求的解，但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。

5. **由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。**支持向量的个数很少，所以支持向量机是由很少的“重要的”训练样本确定的。



## 四、学习的对偶算法

应用拉格朗日对偶性，通过求解对偶问题(dual probelm)得到原始问题(primitive problem)的最优解。

首先构建拉格朗日函数(Lagrange function)。为此，对每一个不等式约束引进拉格朗日乘子(Lagrange multiplier)$\alpha_i\ge0，i=1,2,...N$，定义拉格朗日函数:
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w.x_i+b)+\sum_{i=1}^N\alpha_i
$$
其中，$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$为拉格朗日乘子向量。

根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$
\max_{\alpha}\min_{w,b}L(w,b,\alpha)
$$
所以，为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$\alpha$的极大。

1. 求$\min_{w,b}L(w,b,\alpha)$

   将拉格朗日函数$L(w,b,\alpha)$分别对$w,b$求偏导数并令其等于0。
   $$
   \nabla_w L(w,b,a)=w-\sum_{i=1}^N\alpha_iy_ix_i=0\\
   \nabla_b L(w,b,\alpha)=-\sum_{i=1}^N\alpha_iy_i=0
   $$
   得：
   $$
   w=\sum_{i=1}^N \alpha_iy_ix_i\\
   \sum_{i=1}^N\alpha_iy_i=0
   $$
   代入拉格朗日函数得到：
   $$
   L(w,b,\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j-\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N\alpha_jy_jx_jx_i+b)+\sum_{i=1}^N\alpha_i\\
   =-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j-\sum_{i=1}^N\alpha_iy_ib+\sum_{i=1}^N\alpha_i\\
   =-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j+\sum_{i=1}^N\alpha_i
   $$
   即：
   $$
   \min_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j+\sum_{i=1}^N\alpha_i
   $$

2. 求$\min_{w,b}L(w,b,\alpha)$对$\alpha$的极大，即是对偶问题
   $$
   \max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j+\sum_{i=1}^N\alpha_i\\
   \left.
   \begin{array}{**lr**}
   &\mathrm{s.t.}\quad 
   \sum_{i=1}^N\alpha_iy_i=0\\
   &\alpha_i\ge0,i=1,2,..,N
   \end{array}
   \right.
   $$
   根据已有的定理可以证明，存在$w^*,\alpha^*,\beta^*$，使$w^*$是原始问题的解，$\alpha^*,\beta^*$是对偶问题的解。这意味着求解原始问题可以转化为求解对偶问题。

3. 对于线性可分训练数据集，假设对偶最优化问题对$\alpha$的解为$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$。可以由$\alpha^*$求得原始最优化问题的解$w^*,b^*$。有如下定理：

   **定理**：设$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_N)^T$是对偶最优化问题的解，则存在下标j，使得$\alpha^*_j>0$，并可按下式求得原始问题最优化问题的解$w^*,b^*$：
   $$
   w^*=\sum_{i=1}^N\alpha_i^*y_ix_i\\
   b=y_j-\sum_{i=1}^N\alpha_i^*y_i(x_i.x_j)
   $$
   **证明**：

   因为KTT条件成立，所以：
   $$
   \nabla_wL(w^*,b^*,\alpha^*)=w^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0\\
   \nabla_bL(w^*,b^*,\alpha^*)=-\sum_{i=1}^N\alpha_i^*y_i=0\\
   \alpha_i^*(y_i(w^*.x_i+b^*)-1)=0,i=1,2,...,N\\
   y_i(w^*.x_i+b^*)-1\ge0,i=1,2,...,N\\
   \alpha_i^*\ge0,i=1,2,...,N
   $$
   因此，$w^*=\sum_{i=1}^N\alpha^*_iy_ix_i$。其中，至少有一个$\alpha_j^*>0$(这个反证法就可以证明)。对此有$y_j(w^*.x_j+b^*)-1=0$将$w^*$带入并注意到$y_i^2=1$，所以得到：
   $$
   b^*=y_j-\sum_{i=1}^N\alpha_i^*y_i(x_i.x_j)
   $$

4. 因此，分离超平面可以写成：
   $$
   \sum_{i=1}^N\alpha_i^*y_i(x.x_i)+b^*=0
   $$
   分类决策函数可以写成：
   $$
   f(x)=\mathrm{sign}(\sum_{i=1}^N\alpha_i^*y_i(x.x_i)+b^*)
   $$
   这意味着，**分类决策函数只依赖于输入x和训练样本输入的内积**。这个分类决策函数称为线性可分支持向量机的对偶形式。



## 五、线性可分支持向量机的学习算法

1. **输入**：线性可分训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in \mathcal{X}=R^n,y_i\in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。

2. **输出**：分离超平面和分类决策函数。

3. + 构造并求解约束最优化问题：
     $$
     \min_{\alpha}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_ix_j-\sum_{i=1}^N\alpha_i\\
     \mathrm{s.t.} \quad
     \sum_{i=1}^N\alpha_iy_i=0,\quad\alpha_i\ge0,\quad i=1,2,...,N
     $$
     求得最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T$。

   + 计算
     $$
     w^*=\sum_{i=1}^N\alpha_i^*y_ix_i
     $$
     并选择$\alpha^*$的一个正分量$\alpha_j^*\gt 0$，计算
     $$
     b^*=y_j-\sum_{i=1}^N\alpha_j^*y_i(x_ix_j)
     $$

   + 求得分离超平面$w^*.x+b^*=0$。

     分类决策函数：$f(x)=\mathrm{sign}(w^*.x+b^*)$。

   + 在线性可分支持向量机中，$w^*,b^*$只依赖于训练数据中对应于$\alpha_i^*>0$的样本点$(x_i,y_i)$，而其他样本点对$w^*$和$b^*$没有影响。

     我们**将训练数据中对应于$\alpha_i^*>0$的实例点$x^*\in R^n$称为支持向量**。

4. **支持向量**：考虑原始最优化问题和对偶最优化问题，**将训练数据集中对应于$\alpha_i^*>0$的样本点$(x_i,y_i)$的实例$x_i\in R^n$称为支持向量。**

   根据这一定义，支持向量一定在间隔边界上。由KKT互补条件可知：
   $$
   \alpha_i^*(y_i(w^*.x_i+b^*)-1)=0,\quad i=1,2,...,N
   $$
   对应于$\alpha_i^*>0$的实例$x_i$，有
   $$
   y_i(w^*.x_i+b^*)-1=0
   $$
   即：
   $$
   w^*.x_i+b^*=\pm1
   $$
   即$x_i$一定在间隔边界上，这与之前的支持向量的定义一致。