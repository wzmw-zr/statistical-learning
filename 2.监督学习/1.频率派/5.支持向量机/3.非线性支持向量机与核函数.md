# 非线性支持向量机与核函数

## 一、核技巧

### (1) 非线性分类问题

1. 非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。
2. **非线性可分问题**：对给定的一个训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中实例$x_i$属于输入空间，$x_i\in \mathcal{X}=R^n$，对应的标记有两类$y_i\in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。如果能用$R^n$中的一个超平面将正负例正确分开，则称这个问题是非线性可分问题。
3. 用线性分类方法解决非线性分类问题分为两步：
   + 首先使用一个变换将原空间的数据映射到新空间
   + 然后在新空间中用线性学习分类方法从训练数据中学系分类模型。
4. 核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间(欧氏空间$R^n$或离散集合)对应于一个特征空间(希尔伯特空间$\mathcal{H}$)，使得在输入空间$R^n$中的超曲面模型对应于特征空间$\mathcal{H}$中的超平面问题(支持向量机)，这样，分类问题的学系任务通过在特征空间中求解线性支持向量机就可以完成。



### (2)核函数的定义

1. **核函数**：设$\mathcal{X}$是输入空间(欧氏空间$R^n$的子集或离散集合)，又设$\mathcal{H}$是特征空间(希尔伯特空间)，如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射：$\psi(x)=\mathcal{X}\rightarrow\mathcal{H}$，使得对所有$x,z\in \mathcal{X}$，函数$K(x,z)$满足条件：$K(x,z)=\psi(x).\psi(z)$。则称$K(x,z)$是核函数，$\psi(x)$为映射函数，$\psi(x).\psi(z)$是$\psi(x)$和$\psi(z)$的内积。
2. 核技巧的想法是，**在学习和预测中只定义核函数$K(x,z)$，而不显式地定义映射函数$\psi$。**通常，直接计算$K(x,z)$较容易，而通过$\psi(x)$和$\psi(z)$计算$K(x,z)$并不容易。此外，$\psi$是输入空间$R^n$到特征空间$\mathcal{H}$的映射，特征空间$\mathcal{H}$一般是高维的，甚至是无穷维的。**并且，对于给定的核函数$K(x,z)$，特征空间$\mathcal{H}$和映射函数$\psi$的取法并不唯一，可以取不同的特征空间，即使是在统一特征空间也可以取不同的映射**。



### (3)核技巧在支持向量机中的应用

1. 在线性支持向量机的**对偶问题**中，无论是**目标函数**还是**决策函数**(分离超平面)，都**只涉及输入实例与实例之间的内积。**

2. 在对偶问题的目标函数中的内积$x_i.x_j$可以用核函数$K(x_i,x_j)=\psi(x_i).\psi(x_j)$来代替。此时对偶问题的目标函数成为：
   $$
   W(\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,y_j)-\sum_{i=1}^N\alpha_i
   $$
   同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数市成为：
   $$
   f(x)=\mathrm{sign}(\sum_{i=1}^{N_s}\alpha_i^*y_i\psi(x_i).\psi(x)+b^*)=\mathrm{sign}(\sum_{i=1}^{N_s}\alpha_i^*y_iK(x_i,x)+b^*)
   $$

3. 当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。

4. 也就是说，在核函数$K(x,z)$给定的条件下，可以利用线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间中进行的，不需要显式地定义特征空间和映射函数。



## 二、正定核

1. 通常说的核函数就是正定核函数(positive definite kernel function)。
2. 假设$K(x,z)$是定义在$\mathcal{X}\times \mathcal{X}$上的对称函数，并且对任意的$x_1,x_2,...,x_m \in X$，$K(x,z)$关于$x_1,x_2,...,x_m $的Gram矩阵是半正定的，可以依据函数$K(x,z)$，构成一个希尔伯特空间(Hilbert space)，其步骤是：
   + 首先定义映射$\psi$并构成向量空间$\mathcal{S}$
   + 然后在$\mathcal{S}$上定义内积构成内积空间
   + 最后将$\mathcal{S}$完备化构成希尔伯特空间

### (1)定义映射$\psi$，构成向量空间S

先定义映射$\psi:x\rightarrow K(.,x)$，根据这一映射，对任意$x_i\in \mathcal{X}，\alpha_i\in R,i=1,2,...,m$，定义线性组合$f(.)=\sum_{i=1}^m\alpha_i K(.,x_i)$。考虑由线性组合为元素的集合S，由于集合S对加法和数乘运算是封闭的，所以构成一个向量空间。

### (2)在S上定义内积，使其称为内积空间

在S上定义一个运算$*$：

对任意$f,g\in S$:
$$
f(.)=\sum_{i=1}^m\alpha_iK(.,x_i)\\
g(.)=\sum_{j=1}^l\beta_jK(.,x_i)
$$
定义运算$*$:
$$
f*g=\sum_{i=1}^m\sum_{j=1}^l\alpha_i\beta_jK(x_i,z_j)
$$
可以证明运算*是空间S的内积，为此需要证明：

1. $(cf)*g=c(f*g),c\in R$
2. $(f+g)*h=f*h+g*h,h\in S$
3. $f*g=g*f$
4. $f*f\ge0$，且$f*f=0 \iff f=0$

### (3)将内积空间S完备化为希尔伯特空间

1. 将内积空间S完备化。由定义的内积可以得到范数$||f||=\sqrt{f.f}$。因此，S是一个赋范向量空间。根据泛函分析理论，对于不完备的赋范向量空间S，一定可以使之完备化，得到完备的赋范向量空间$\mathcal{H}$。一个内及空间，当作为一个赋范向量空间是完备的时候，就是希尔伯特空间。
2.  这样的希尔伯特空间称为再生核希尔伯特空间(reproducing kernel Hilbert space, RKHS)。这是由于核有可再生性，即满足$K(.,x).f=f(x)$，及$K(.,x).K(.,z)=K(x,z)$，称为再生核。

### (4)正定核的充要条件

1. **定理(正定核的充要条件)**：设$K:\mathcal{X}\times \mathcal{X}\rightarrow R$是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_i \in \mathcal{X},i=1,2,...,m$，$K(x,z)$对应的Gram矩阵$K=[K(x_i,x_j)]_{m\times m}$是半正定矩阵。
2. **正定核的等价定义**：设$\mathcal{X}\in R^n,K(x,z)$是定义在$\mathcal{X}\times\mathcal{X}$上的对称函数，如果对任意$x_i\in \mathcal{X} ，i=1,2,...,m,K(x,z)$对应的Gram矩阵$K=[K(x_i,x_j)]_{m\times m}$是半正定矩阵，则称$K(x,z)$是正定核。



## 三、常用核函数

### (1)多项式核函数(polynomial kernel function)

$K(x,z)=(x.z+1)^p$，对应的支持向量机是一个p次多项式分类器，在此情形下，分类决策函数成为:
$$
f(x)=\mathrm{sign}(\sum_{i=1}^{N_s}a_i^*y_i(x_i.x+1)^p+b^*)
$$

### (2)高斯核函数(Gaussian kernel function)

$$
K(x,z)=exp{(-\frac{||x-z||^2}{2\sigma ^2})}
$$

对应的支持向量机是搞死径向基函数(radial basis function)分类器，在此情形下，分类决策函数成为：
$$
f(x)=\mathrm{sign}(\sum_{i=1}^{N_s}a_i^*y_iexp(-\frac{||x-x_i||^2}{2\sigma^2})+b^*)
$$

### (3)字符串核函数(string kernel function)

核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。比如，字符串核函数是定义在字符串上的核函数。

字符串核函数一般在文本分类、信息检索、生物信息学等方面应用。

两个字符串s和t上的字符串核函数是基于映射$\phi_n$的特征空间中的内积：
$$
k_n(s,t)=\sum_{u\in \sum^n}[\phi(s)]_u[\phi(t)]_u
$$
字符串核函数$k_n(s,t)$给出了字符串s和t中长度等于n的所有字串组成的特征向量的余弦相似度(cosine similarity)。**字符串核函数可以由动态规划快速计算**。



## 四、非线性支持向量分类机

1. **非线性支持向量机**：从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划，学习得到分类决策函数：
   $$
   f(x)=\mathrm{sign}(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*)
   $$
   称为非线性支持向量机，$K(x,z)$是正定核函数。

2. **非线性支持向量机的学习算法**：

   + **输入**：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i\in \mathcal{X}=R^n,y_i\in \{+1,-1\},i=1,2,..,N$;

   + **输出**：分类决策函数

   + + 选取适当的核函数$K(x,z)$和适当的参数C，构造并求解最优化问题
       $$
       \min_{\alpha}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i\\
       s.t.\quad\sum_{i=1}^N\alpha_iy_i=0\\
       0\le\alpha_i\le C,i=1,2,..,N
       $$
       求得最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...\alpha_N^*)^T$。

     + 选择$\alpha^*$的一个正分量$0<\alpha_j^*<C$，计算
       $$
       b^*=y_j-\sum_{i=1}^N\alpha_i^*y_iK(x_i,x_j)
       $$

     + 构造决策函数：
       $$
       f(x)=\mathrm{sign}(\sum_{i=1}^N\alpha_i^*y_iK(x,x_i)+b^*)
       $$

     当$K(x,z)$是正定核函数时，这是一个凸二次规划问题。

