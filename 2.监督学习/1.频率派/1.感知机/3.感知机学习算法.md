# 感知机学习算法

1. 感知机学习问题转化为**求解损失函数式的最优化问题**，最优化的方法是**随机梯度下降法**。
2. 感知机学习的具体算法包括原始形式和对偶形式。
3. **在训练数据线性可分条件下感知及学习算法具有收敛性。**



## 一、感知机学习算法的原始形式

1. 感知机学习算法是对给定训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中，$x_i\in X=R^n,y\in Y = {+1,-1},i=1,2,..N$，求参数$w,b$，使其为下面损失函数极小化问题的解：
   $$
   \min_{w,b}L(w,b)=-\sum_{x_i \in M}y_i(w.x_i+b)
   $$
   其中M为误分类点的集合。

2. 感知机学习算法是误分类驱动的，具体采用随机梯度下降法(stochastic gradient descent)。

   首先，任意选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程中不是一次使M中所有误分类点的梯度下降，而是**一次随机选取一个误分类点使其梯度下降**。

3. 假设误分类点结合M是固定的，那么损失函数$L(w,b)$的梯度由
   $$
   \bigtriangledown_wL(w,b)=-\sum_{x_i\in M}y_ix_i\\
   \bigtriangledown_bL(w,b)=-\sum_{x_i\in M}y_i
   $$
   给出。

   随机选取一个误分类点，对w，b进行更新：
   $$
   w \leftarrow w + \eta y_ix_i\\
   b \leftarrow b + \eta y_i
   $$
   式子中$\eta(0<\eta \le1)$是步长，在统计学习中又称为**学习率(learning rate)**，这样通过迭代可以期待$L(w,b)$不断减小，直到0.

4. **感知机学习算法的原始形式：**

+ **输入：**训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中，$x_i\in X=R^n,y\in Y = {+1,-1},i=1,2,..N$; 学习率$\eta$。
+ **输出：**$w,b$，感知机模型$f(x)=sign(w.x+b)$。
+ 1.  选取初值$w_0,b_0$;
  2. 在训练集中选取数据$(x_i,y_i)$。
  3. 如果$y_i(w.x_i+b)\le 0$，则$w \leftarrow w+\eta y_ix_i, b \leftarrow b+\eta y_i$。
  4. 转至2，直至训练集中没有误分类点。
+ 感知机学习算法的原始形式直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，**使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面之间的距离，直至超平面点越过该误分类点使其被正确分类。**



## 二、感知机学习算法的收敛性

1. **结论：**对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分类超平面及感知机模型。

2. 讲偏置b并入权重向量w，记作$\hat{w}=(x^T,b)^T$，同样也将输入向量加以扩充，加进常数1,记作$\hat{x}=(x^T,1)^T$。这样，$\hat{x}\in R^{n+1},\hat{w}\in R^n+1$，显然$\hat{w}.\hat{x}=w.x+b$。

3. **Novikoff定理：**设训练数据集$T=\{(x_1,y_1),(x_2,y_2),..,(x_N,y_N)\}$是线性可分的，其中$x_i\in X = R^n,y_i\in Y = {+1,-1},i=1,2,...,N$，则：

   (1) 存在满足条件$||\hat{w}_{opt}||=1$的超平面$\hat{w}_{opt}.\hat{x}=w_{opt}.x+b_opt=0$将训练数据集完全正确分开，且存在$\gamma>0$，对所有$i=1,2..N$：
   $$
   y_i(\hat{w}_{opt}.\hat{x})=y_i(w_{opt}.x_i+b_{opt})\ge \gamma
   $$
   (2) 令$R=\max_{1\le i \le N}||\hat{x}_i||$，则感知机算法在训练数据集上的误分类次数k满足不等式
   $$
   k\le(\frac{R}{\gamma})^2
   $$

   > 证明：
   >
   > (1) 由于训练数据集是线性可分的，则存在超平面可以将训练数据集完全正确分开，取此超平面为$\hat{x}_{opt}.\hat{x}=w_{opt}.x+b_{opt}=0$，使$||\hat{x}_{opt}||=1$。由于对有限的$i=1,2,...N$，均有
   > $$
   > y_i(\hat{x}_{opt}.\hat{x}_i)=y_i(w_{opt}.x+b_{opt})>0
   > $$
   > **正是因为训练数据集数据是有限的，**所以存在
   > $$
   > \gamma = \min_{i}\{y_i(w_{opt}.x_i+b_{opt})\}
   > $$
   > 使$y_i(\hat{w}_{opt}.\hat{x})=y_i(w_{opt}.x_i+b_{opt})\ge \gamma$。
   >
   >  
   >
   > (2) 感知机算法从$\hat{w}_0=0$开始，如果实例被误分类，则更新权重。令$\hat{w}_{k-1}$是第k个误分类实例之前的扩充权重向量，即
   > $$
   > \hat{w}_{k-1}=(w_{k-1}^T,b_{k-1})^T
   > $$
   > 则第k个误分类实例的条件是
   > $$
   > y_i(\hat{w}_{k-1}.\hat{x}_i)=y_i(w_{k-1}.x_i+b_{k-1})\le 0
   > $$
   > 若$(x_i,y_i)$是被$\hat{w}_{k-1}=(w_{k-1}^T,b_{k-1})^T$误分类的数据，则$w$和$b$的更新是
   > $$
   > w_k \leftarrow w_{k-1}+\eta y_i x_i\\
   > b_k \leftarrow b_{k-1}+\eta y_i
   > $$
   > 即$\hat{w}_k=\hat{w}_{k-1}+\eta y_i \hat{x}_i$。
   >
   >  
   >
   > 为了证明$k\le (\frac{R}{\gamma})^2$，需要证明下面两个不等式：
   > $$
   > \hat{w}_k.\hat{w}_{opt}\ge k\eta\gamma\\
   > ||\hat{w}_k||^2 \le k \eta^2\gamma^2
   > $$
   > 首先，我们可以证明第一个不等式：
   > $$
   > \hat{w}_k.\hat{w}_{opt}=\hat{w}_{k-1}.\hat{w}_{opt}+\eta y_i \hat{w}_{opt}.\hat{x}_i\\
   > \ge\hat{w}_{k-1}.\hat{w}_{opt}+\eta\gamma
   > $$
   > 由此递推即可得到不等式：
   > $$
   > \hat{w}_k.\hat{w}_{opt}\ge \hat{w}_{k-1}.\hat{w}_{opt}+\eta\gamma \ge \hat{w}_{k-2}.\hat{w}_{opt}+2\eta\gamma \ge ...\ge k\eta\gamma
   > $$
   > 下面证明第二个不等式:
   > $$
   > ||\hat{w}_{k}||^2=||\hat{w}_{k-1}||^2+2\eta y_i\hat{w}_{k-1}.\hat{x}_i+\eta^2||x_i||^2\\
   > \le ||\hat{w}_{k-1}||^2+\eta^2||\hat{x}_i||^2\\
   > \le ||\hat{w}_{k-1}||^2+\eta^2R^2\\
   > \le ||\hat{w}_{k-2}||^2+2\eta^2R^2 \le ...\\
   > \le k\eta^2R^2
   > $$
   > 因此，可以得到$k \le (\frac{R}{\gamma})^2$。

4. 这个定理表明，**误分类的次数k是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面**。

   也就是说，**当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的**。

   但是，当训练数据集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。

5. 此外，感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件，这是线性支持向量机的做法。



## 三、感知机学习算法的对偶形式

1. + 感知机学习算法的对偶形式的基本想法：**将w和b表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得w和b**。

   + 在算法中可以假设初始值$w_0,b_0$均为0，对误分类点$(x_i,y_i)$，通过
     $$
     w \leftarrow w+\eta y_ix_i\\
     b \leftarrow b + \eta y_i
     $$
     逐步修改w，b，设修改n次，则w，b关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha_iy_i$，这里$\alpha_i=n_i\eta$，$n_i$是点$(x_i,y_i)$被误分类的次数。

     这样，最后学习到的w和b可以分别表示为：
     $$
     w=\sum_{i=1}^N\alpha_iy_ix_i\\
     b=\sum_{i=1}^N\alpha_iy_i
     $$
     这里$\alpha_i=n_i\eta\ge0,i=1,2...N$，$n_i$表示第i个实例点由于误分类而进行更新的次数。

     **实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。**换句话说，这样的实例对学习结果影响最大。

2. **感知机学习算法的对偶形式**：

   + **输入**：线性可分的数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，其中$x_i\in R^n，y_i \in {+1,-1},i=1,2,...N$; 学习率$\eta,0<\eta\le1$。

   + **输出**：$\alpha,b$;感知机模型$f(x)=sign(\sum_{j=1}^N\alpha_jy_jx_j.x+b)$，其中$\alpha=(\alpha_1,\alpha_2,...\alpha_N)^T$。

   + (1)$\alpha \leftarrow 0, b \leftarrow 0$;

     (2)在训练集中选取数据$(x_i,y_i)$;

     (3)如果$y_i(\sum_{j=1}^N\alpha_jy_jx_j.x_i+b)\le 0$，则
     $$
     \alpha_i\leftarrow \alpha_i+\eta\\
     b\leftarrow b+\eta y_i
     $$
     (4)转至(2)直到没有误分类数据。

3. 对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中实例见的内积计算出来并以矩阵的形式存储，这个矩阵就是Gram矩阵(Gram matrix)，$G=[x_i.x_j]_{N\times N}$。