# AdaBoost算法的解释

AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习算法。

## 一、加法模型(additive model)

考虑加法模型(additive model):
$$
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。

在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化，即损失函数极小化问题:
$$
\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
$$
因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度，具体地，每步只需优化如下损失函数：
$$
\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))
$$


## 二、前向分步算法

1. **输入**：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$，损失函数$L(y,f(x))$，基函数集$\{b(x;\gamma\}$。

2. **输出**：加法模型$f(x)$。

3. + 初始化$f_0(x)=0$;

   + 对$m=1,2,...,M$：

     + 极小化损失函数：
       $$
       (\beta_m,\gamma_m)=\arg \min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
       $$
       得到参数$\beta_m,\gamma_m$。

     + 更新
       $$
       f_m(x)=f_{m-1}(x)+\beta_nb(x;\gamma_m)
       $$

   +  得到加法模型：
     $$
     f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
     $$
     这样，前向分步算法将同时求解$m=1$到M的所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解各个$\beta_m,\gamma_m$的优化问题。



## 三、前向分步算法与AdaBoost

1. **定理**：AdaBoost算法是前向分步算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。

   > **证明**：
   >
   > 设AdaBoost的最终分类器：
   > $$
   > f(x)=\sum_{m=1}^M\alpha_mG_m(x)
   > $$
   > 由基本分类器$G_m(x)$及其系数$\alpha_m$组成，$m=1,2,...,M$。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。可以证明，使用指数函数作为损失函数的前向分布算法与AdaBoost等价。