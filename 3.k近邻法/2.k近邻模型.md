# k近邻模型

k近邻法使用的模型实际上对应于特征空间的划分。

模型由三个基本要素：距离度量、k值选择和分类决策规则决定。



## 一、模型

1. k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定。

   这实际上是根据上述要素将**特征空间划分为一些子空间**，**确定子空间里的每个点所属的类**。

2. 特征空间中，对每个训练实例点$x_i$，距离该点比其他点更近的所有点组成一个区域，叫做单元(cell)。

   每个训练实例点拥有一个单元，所有训练实例点构成对特征空间的一个划分。

3. 最邻近法将实例$x_i$的类$y_i$作为其单元中所有点的类标记(class label)，这样，每个单元的实例点的类别是确定的。



## 二、距离度量

1. 特征空间中两个实例点的距离是两个实例点相似程度的反映。

2. k近邻模型的特征空间一般是n维实数向量空间$R^n$，使用的距离是欧氏距离，但也可以是其他更一般的$L_p$距离($L_p$ distance)。

3. 设特征空间X是n维实数向量空间$R^n$，$x_i,x_j \in X,x_i=(x_i^{(1)},x_i^{(2)},...x_i^{(n)})^T$。$x_i,x_j$的$L_p$距离定义为：
   $$
   L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
   $$
   这里$p\ge1$。

   当$p=2$时，称为欧式距离(Euclidean distance)，即
   $$
   L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}
   $$
   当$p=1$时，称为曼哈顿距离(Manhattan distance)，即
   $$
   L_1(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|
   $$
   当$p=\infty$时，称为契比雪夫距离，即各个坐标距离的最大值：
   $$
   L_{\infty}(x_i,y_j)=\max_{l}|x_i^{(l)}-x_j^{(l)}|
   $$
   

## 三、k值的选择

1. k值的选择会对k近邻法的结果产生重大影响。

2. 选取较小的k值：

   + 优点：
     + 用较小的邻域中的训练实例进行预测
     + **学习的近似误差(approximation error)就会减小**
     + 只有与输入实例较近的(相似的)训练实例才会对预测结果起作用。

   + 缺点：
     + **学习的估计误差(estimation error)会增大**，预测结果会对邻近的实例点非常敏感。
     + 如果邻近的实例点恰好是噪声，预测就会出错。
     + 换句话说，**k值的减小就意味着整体模型变得复杂，容易发生过拟合**。

3. 选取较大的k值：

   + 优点：
     + 相当于用较大的邻域中的训练实例进行预测。
     + 可以**减小学习的估计误差**。
   + 缺点：
     + 学习的**近似误差会增大**
     + 这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。
     + k值的增大就意味着整体的模型变得简单。

4. 在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。



## 四、分类决策规则

1. k近邻法中的分类规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。

2. 多数表决规则(majority voting rule)有如下解释：如果分类的损失函数为0-1损失函数，分类函数为
   $$
   f:R^n\rightarrow \{c_1,c_2,..,c_K\}
   $$
   那么误分类的概率是
   $$
   P(Y\neq f(X))=1-P(Y=f(X))
   $$
   对给定的实例$x\in X$其最邻近的k个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是
   $$
   \frac{1}{k}\sum_{x_i \in N_k(x)}I(y_i\neq c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)
   $$
   上面的式子是显然的，简单解释就是这k个实例点中可以正确分类的个数占比与误分类个数占比之间的关系。

3. 要使误分类率最小，即经验风险最小，就要使$\sum_{x_i\in N_k(x)}I(y_i=c_j)$最大，所以多数表决等价于经验风险最小化。